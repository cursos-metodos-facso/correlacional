---
title: "Práctico 3: Correlación"
date: "2024-10-01"
lang: es
execute:
  freeze: auto
  cache: false 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F,
                      error = F, 
                      message = F) 
```

# Objetivo de la práctica

El objetivo de esta guía práctica es introducirnos al cálculo e interpretación del coeficiente de correlación de Pearson. Además, aprenderemos a emplear la correlación en inferencia estadística, interpretar sus tamaños de efecto, limitaciones y otras medidas de correlación entre variables.

La guía tiene **3 ejercicios**. El primero de ellos es un ejemplo, y los ejercicios 2 y 3 se desarrollan de manera autónoma en la sala (también puede ser en grupo). 

## Recursos de la práctica

En esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por [COES](https://coes.cl/encuesta-panel/). Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace: [{{< fa table >}} `ELSOC 2016`](https://multivariada.netlify.app/assignment/data/proc/ELSOC_ess_merit2016.RData). Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016.

# Preparación datos

Comencemos por preparar nuestros datos. Iniciamos cargando las librerías necesarias.

```{r librerias, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
pacman::p_load(tidyverse, # Manipulacion datos
               sjPlot, # Graficos
               kableExtra, #Tablas
               broom, # Manipulacion
               gtools) # Manipulacion

options(scipen = 999) # para desactivar notacion cientifica
rm(list = ls()) # para limpiar el entorno de trabajo
```

Cargamos los datos directamente desde internet (por esta vez).

```{r datos, echo=TRUE, message=FALSE, warning=FALSE}

load(url("https://multivariada.netlify.app/assignment/data/proc/ELSOC_ess_merit2016.RData")) #Cargar base de datos

```

A continuación, exploramos la base de datos **proc_elsoc**.

```{r exploracion, echo=TRUE, message=FALSE, warning=FALSE}

names(proc_elsoc) # Nombre de columnas
dim(proc_elsoc) # Dimensiones

```
Contamos con 7 variables (columnas) y 2927 observaciones (filas).

# Correlación de Pearson

::: callout-note
**¿Qué era la correlación?**

La correlación es una medida de asociación entre variables, que describe el sentido (dirección) y fuerza de la asociación.

En otras palabras, nos permite conocer cómo y cuánto se relaciona la variación de una variable, con la variación de otra variable.

:::

La correlación producto-momento de Pearson es una medida estandarizada de covarianza que nos muestra la asociación **lineal** (sentido y fuerza) entre dos variables continuas. 

$$
Correlacion = r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})} {(n-1)\sigma_x \sigma_y }
$$

Sus valores oscilan entre -1 y 1, donde:

* $r$ = 1: Correlación positiva perfecta. Cuando una variable aumenta, la otra también aumenta.

* $r$ = -1: Correlación negativa perfecta. Cuando una variable aumenta, la otra disminuye.

* $r$ = 0: No hay correlación lineal entre las variables. No hay una relación lineal discernible entre los cambios en las variables.

Cuanto más cercano esté el valor de r a 1 o -1, más fuerte será la correlación. Cuanto más cercano esté a 0, más débil será la correlación.

![](../slides/img/scatters3.png)

Para obtener el coeficiente de correlación directamente en R se utiliza la función `cor()` :

```{r ex0_cor, warning=FALSE}
cor(x = proc_elsoc$mesfuerzo, y = proc_elsoc$mtalento, use = "complete.obs")
```

Tenemos que la correlación entre la variable de `mesfuerzo` (percepción de meritocracia asociada al esfuerzo) y `mtalento` (percepción de meritocracia asociada al talento) es de 0.69. 

Siempre es recomendable acompañar el valor de la correlación con una exploración gráfica de la distribución bivariada de los datos. El gráfico o diagrama de dispersión es una buena herramienta, ya que muestra la forma, la dirección y la fuerza de la relación entre dos variables cuantitativas.

```{r, echo=TRUE, warning=FALSE}
sjPlot::plot_scatter(data = proc_elsoc, x = mesfuerzo, y = mtalento)
```

En el siguiente [enlace](https://rpsychologist.com/correlation/) pueden visualizar la correlación para dos variables cambiando la fuerza y el sentido de esta, al mismo tiempo que les permite observar la varianza compartida entre ambas variables.

# Tamaños de efecto

¿Y cómo puedo saber si el valor de la correlación es alto, medio o bajo? Si bien la correlación no nos indica causalidad, si nos permite conocer la dirección y fuerza de asociación entre dos variables. Un estándar para determinar qué tan fuerte es dicha asociación en las ciencias sociales es el propuesto por Cohen (1998).

<style>
  table {
    margin-left: auto; /* Ajustamos el margen izquierdo a automático */
    margin-right: auto; /* Ajustamos el margen derecho a automático */
    border-collapse: collapse;
    width: 60%;
    border: 2px solid black;
  }
  
  th, td {
    border: 1px solid #D3D3D3;
    padding: 8px;
    text-align: center;
  }
</style>

<table>
  <tr>
    <th class="cell-left">r</th>
    <th class="cell-left">Significado aproximado (Cohen 1988)</th>
  </tr>
  <tr>
    <td class="cell-left">&lt; ±0.1&emsp;</td>
    <td class="cell-left">Muy pequeño</td>
  </tr>
  <tr>
    <td class="cell-left">±0.1–0.3</td>
    <td class="cell-left">Pequeño</td>
  </tr>
  <tr>
    <td class="cell-left">±0.3–0.5</td>
    <td class="cell-left">Moderado</td>
  </tr>
  <tr>
    <td class="cell-left">&gt;±0.5</td>
    <td class="cell-left">Grande</td>
  </tr>
</table>


Con estos criterios podemos interpretar de mejor manera nuestros resultados de correlación. Como se observa, mientras más alto (sea en + o -) el coeficiente, más juntos estarán los datos (puntos), mostrando un patrón. 

```{r correlation-grid, echo=FALSE, out.width="80%", fig.align='center'}
make_correlated_data <- function(r, n = 200) {
  MASS::mvrnorm(n = n, 
                mu = c(0, 0), 
                Sigma = matrix(c(1, r, r, 1), nrow = 2), 
                empirical = TRUE) %>% 
    magrittr::set_colnames(c("x", "y")) %>% 
    as_tibble()
}

cor_grid <- tibble(r = c(0.2, 0.4, 0.7, 0.9)) %>% 
  mutate(data = map(r, make_correlated_data)) %>% 
  unnest(data)

ggplot(cor_grid, aes(x = x, y = y)) +
  geom_point(size = 2, color = "white", fill = "black", pch = 21) +
  facet_wrap(vars(r), labeller = label_both) +
  # theme_minimal() +
  theme(strip.text = element_text(face = "bold", size = rel(1.3), hjust = 0))
```

:::::: {.row .d-flex .justify-content-center}
::::: {.col-md-6} 
:::: {.card .bg-danger .text-white}
::: {.card-body}

**Interpretación**

Recordemos nuestro resultado al comienzo:

Tenemos que la correlación entre la variable de percepción de esfuerzo y talento es 0.7. ¿Cómo interpreto esto?

Una manera recomendable es la siguiente:

_El coeficiente de correlación de Pearson entre esfuerzo y talento es positivo y grande (r = 0.7) según Cohen (1988)._ 


:::
::::
:::::
::::::


# Inferencia en correlación

En el contexto de la inferencia, **la correlación** nos permite **determinar si existe (o no) una asociación estadísticamente significativa** entre dos variables. En ese sentido, la lógica del contraste de hipótesis usando correlación es:

::: {.callout-note}
#### Hipótesis en correlación

Contrastamos la _hipótesis nula_ (o de trabajo) de _no_ asociación entre variables:
$$  H_{0}: \rho = 0 $$

En relación a una _hipótesis alternativa_ sobre la existencia una asociación significativa entre variables:

$$  H_{A}: \rho \neq 0 $$
:::

Tomemos por ejemplo la siguiente pregunta de investigación: ¿en qué medida el nivel educacional alcanzado por las personas se relaciona con su estatus social subjetivo en Chile en el 2016?

Formulemos nuestra hipótesis:

- $H_{0}$: $cor(educ,ess)$ $=$ $0$

- $H_{A}$: $cor(educ,ess)$ $\neq$ $0$

Obtengamos el coeficiente de correlación $r$ de Pearson entre el nivel educacional alcanzado (`educ`) y el estatus social subjetivo (`ess`) de las personas en Chile en 2016. Para esto usaremos solamente observaciones completas (_listwise_).

```{r, echo=TRUE}

cor_results <- cor.test(proc_elsoc$edcine, proc_elsoc$ess, 
         method = "pearson", 
         use = "complete.obs")

cor_results
```

Tenemos nuestro resultado, pero es poco amigable visualmente. Generemos una tabla para nuestra correlación.

```{r, echo=TRUE, warning=FALSE}
stats.table <- tidy(cor_results)

stats.table %>% 
    dplyr::mutate(estimate = round(estimate, 2),
                  statistic = round(statistic, 2),
                  ic_95 = paste0("[", round(conf.low, 2), ",", round(conf.high,2), "]"),
                  stars = gtools::stars.pval(p.value),
                  p_value = case_when(p.value < 0.05 & p.value > 0.01 ~ "< 0.05",
                                      p.value < 0.01 & p.value > 0.001 ~ "< 0.01",
                                      p.value < 0.001 ~ "< 0.001",
                                      TRUE ~ ""),
                 p_value = paste0(p_value, stars)) %>% 
    dplyr::select(estimate, statistic, p_value, parameter, method, alternative, ic_95) %>% 
    kableExtra::kable(col.names = c("Estimación", "t", "p-value", "df", "Método", "Alternativa", "95% IC"),
                      booktabs = T) %>% 
    kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                              full_width = T, 
                              latex_options = "hold_position", 
                              position = "center") 
```

Ya sabemos interpretar una correlación, ahora usemos este resultado para probar nuestra hipótesis: 

> El coeficiente de correlación de Pearson entre nivel educativo y el estatus social subjetivo es positivo, estadísticamente significativo (r = 0.28, p < .001) y moderado de acuerdo con las recomendaciones de Cohen (1988). Por tanto, con un 95% de confianza se puede rechazar la $H_{0}$ de no asociación entre variables, existiendo evidencia a favor de la $H_{A}$ sobre una asociación significativa entre nivel educacional y estatus social subjetivo.


# Coeficiente de determinación ($R^2$)

El coeficiente de determinación $R^2$ es una medida estadística que indica la proporción de la varianza total de una variable que es explicada por otra(s) variable(s). En pocas palabras, 

- se utiliza para evaluar cuánta de la variabilidad de una variable se debe a otra variable. 
- sus valores van desde 0 a 1, en donde 0 indica que ambas variables comparten el 0% de su varianza, y 1 que comparten el 100% de su varianza. 


En el contexto de la correlación entre **solo dos variables**, el $R^2$ es igual a elevar al cuadrado el coeficiente de correlación = `(r)^2`. Esto nos permite conocer qué tanto la variabilidad de una variable X estaría asociado a la variabilidad de otra variable Y. 

En nuestro ejemplo anterior entre estatus social subjetivo y años de educación, teníamos que su coeficiente de correlación era _r = 0.3_.

```{r, echo=TRUE, warning=FALSE, message=FALSE}

coef_r <- cor_results$estimate

coef_r
```

Calculemos el $R^2$ de esta asociación.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
coef_r^2
```

Con esto, podemos decir que el 8% de la variabilidad del estatus social subjetivo es compartido con la variabilidad en los años de educación.


# Limitaciones de la correlación de Pearson

El coeficiente de correlación de Pearson tiene una serie de limitaciones:

- medida de asociación lineal entre variables
- no captura apropiadamente asociaciones no lineales
- posee supuestos distribucionales de x e y (distribución normal)
- sensible a valores extremos
- un mismo coeficiente puede reflejar distintas distribuciones bivariadas

Una buena forma de ejemplificar la limitación de que un mismo coeficiente puede reflejar distintas distribuciones es el cuarteto de Anscombe. Este es una demostración respecto a la importancia de visualizar los datos y fue realizada por el estadístico Francis John Anscombe en 1973. El cuarteto de Anscombe comprende cuatro conjunto de datos, todos con las mismas propiedades estadísticas (media, mediana y varianza), pero con diferencias claras en su distribución.

```{r, echo = FALSE}

library(ggplot2)
library(datasets)
library(dplyr)

# Crear un dataframe con los nombres de las series y calcular los coeficientes de correlación
df_anscombe <- data.frame(
  x = c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4),
  y = c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4),
  dataset = rep(c("I", "II", "III", "IV"), each = nrow(anscombe))
)

# Calcular el coeficiente de correlación por grupo
correlations <- df_anscombe %>%
  group_by(dataset) %>%
  summarise(correlation = cor(x, y))

# Unir los coeficientes de correlación con el dataframe
df_anscombe <- df_anscombe %>%
  left_join(correlations, by = "dataset")

# Graficar usando ggplot2
ggplot(df_anscombe, aes(x = x, y = y, color = dataset)) +
  geom_point(size = 2) + 
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +  # Línea de regresión
  facet_wrap(~ dataset, ncol = 2) +  # Crear un gráfico por cada dataset
  theme_minimal() +
  labs(title = "Cuarteto de Anscombe",
       subtitle = "Mismo coeficiente de correlación, pero diferentes patrones",
       x = "X",
       y = "Y") +
  geom_text(aes(label = paste("r = ", round(correlation, 2))), 
            x = 15, y = 5, size = 4, hjust = 1, color = "black") +  # Añadir correlación en cada panel
  scale_color_manual(values = c("I" = "red", "II" = "blue", "III" = "green", "IV" = "orange")) +  # Colores diferentes
  theme(
    aspect.ratio = 1,  # Ajustar la relación de aspecto para hacer los paneles más pequeños
    panel.spacing = unit(0.5, "lines"),  # Reducir el espacio entre paneles
    plot.title = element_text(size = 14),
    plot.subtitle = element_text(size = 12),
    legend.position = "none"
  )

```

Al tener las mismas propiedades estadísticas, los cuatro conjunto de datos dan por resultado el mismo coeficiente de correlación ($r = 0.82$). Tomando los criterios de Cohen (1988) sabemos que esto significa que las variables tienen una asociación lineal fuerte y positiva. Por lo demás, al tener el mismo coeficiente de correlación, esperaríamos que las distribuciones de los datos fueran más o menos parecidas. Sin embargo, esto es justamente lo que nos invita a reflexionar la demostración de Anscombe: **que distintos conjuntos de datos tengan el mismo coeficiente de correlación, no significa que tengan la misma distribución bivariada.**

La demostración del cuarteto de Anscombre nos recuerda la importancia de acompañar los reportes de estadísticos (p.ej el coeficiente de correlación) con visualizaciones de datos.

# Otros tipos de correlación: Spearman

Cuando queremos conocer la asociación entre variables que son ordinales y/o cuando nuestras variables no cumplen con los supuestos de distribución normal, podemos utilizar la **correlación de Spearman**.

- El coeficiente de correlación de Spearman es una medida estadística que evalúa la relación entre variables al considerar no solo la relación lineal entre ellas, sino también su relación de orden.
- Emplea rangos en lugar de valores numéricos para evaluar la relación.
- Sus valores están entre -1 y 1.
- Es alta cuando las observaciones tienen un ranking similar.

En R calcularlo es sencillo, pero _debemos tener en cuenta que las variables que relacionemos tengan un orden de rango similar_: por ejemplo, que el valor más bajo sea el rango más bajo y que el valor más alto sea el rango más alto. 

Tomemos por ejemplo las variables `mesfuerzo` y `mtalento`.

```{r, echo=TRUE, warning=FALSE, message=FALSE, collapse=FALSE}

sjmisc::frq(proc_elsoc$mesfuerzo)

sjmisc::frq(proc_elsoc$mtalento)


```


Ahora, calculemos el coeficiente de correlación de Spearman con `cor.test`.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
cor.test(proc_elsoc$mesfuerzo, proc_elsoc$mtalento, 
         use = "complete.obs",
         method = "spearman") #especificamos metodo spearman
```

Ahora conocemos el valor del coeficiente mediante al argumento `rho`, que es igual a 0.7, siendo positivo y grande según los criterios de Cohen (1988). 

# Ejercicios

## Ejercicio 1

A partir de la base de datos de `proc_elsoc` responda la siguiente pregunta **_¿en qué medida la edad de las personas está asociadas a sus percepciones sobre la meritocracia?_** Para responder esta pregunta siga los siguientes pasos:

1. Calcule el coeficiente de correlación $r$ entre las variables `edad` y `pmerit`. `pmerit` es una variable que promedia las variables de `mesfuerzo` y `mtalento`
2. Interprete el tamaño de efecto del coeficiente siguiendo los criterios de Cohen (1988).
3. Reporte el sentido de la dirección de la correlación
4. Interprete la significancia estadística del coeficiente
5. Visualice la relación entre las variables con un gráfico de dispersión y comente.

## Ejercicio 2
¿Spearman? Comentario Martín: Aquí me queda la duda, yo utilizaría `mesfuerzo` y `mtalento` para hacer el ejemplo de Spearman y buscaría otra variable numerica en ELSOC original para hacer el ejemplo de Pearson (probablemente ingresos)
