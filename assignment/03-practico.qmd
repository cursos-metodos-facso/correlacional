---
title: "Práctico 3: Correlación de Pearson"
date: "2025-10-01"
lang: es
execute:
  freeze: auto
  cache: false 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    warning = F,
    error = F,
    message = F
)
```

# Objetivo de la práctica

El propósito de esta guía es introducir el coeficiente de correlación de Pearson como una herramienta para la investigación, enfocándonos en su aplicación en R. A lo largo del ejercicio, aprenderemos a:

1. Preparar los datos,
2. Estimar el coeficiente de Pearson en R,
3. Interpretar el tamaño del efecto, 
4. Visualizar la relación entre variables mediante gráficos,
5. Aplicar la correlación en inferencia estadística,
6. El rol del coeficiente de determinación, y
7. Reconocer una las principales limitaciones del coeficiente de Pearson.

Utilizaremos un ejemplo que desarrollaremos progresivamente para ilustrar cada paso. Al finalizar, se propondrá un ejercicio autónomo que deberá resolverse de manera individual o grupal, aplicando los conceptos vistos en clases y en esta guía.

## Recursos de la práctica

En esta práctica trabajaremos con los datos del Estudio Longitudinal Social de Chile (ELSOC) del año 2021, elaborado por [COES](https://coes.cl/encuesta-panel/). Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también es posible acceder a la misma información a través del siguiente enlace: [{{< fa table >}} `ELSOC 2021`](https://dataverse.harvard.edu/file.xhtml?fileId=6160180&version=1.0). Desde allí, se puede descargar el archivo que contiene la base de datos ELSOC 2021.

# 1. Preparación de datos

Comencemos por preparar nuestros datos. Iniciamos cargando las librerías necesarias.

```{r librerias, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
pacman::p_load(tidyverse, # Manipulacion datos
               sjPlot, # Graficos
               rstatix, # Test estadísticos
               labelled, # Para labels
               broom, # Tablas
               kableExtra) # Tablas

options(scipen = 999) # para desactivar notacion cientifica
rm(list = ls()) # para limpiar el entorno de trabajo
```

Cargamos los datos directamente desde internet.

```{r datos, echo=TRUE, message=FALSE, warning=FALSE}

# Cargar bbdd pública de ELSOC
load(url("https://dataverse.harvard.edu/api/access/datafile/6160180"))

```

A continuación, exploramos la base de datos `elsoc_2021`.

```{r exploracion, echo=TRUE, message=FALSE, warning=FALSE}

names(elsoc_2021) # Nombre de columnas
dim(elsoc_2021) # Dimensiones

```


Ahora, realizaremos un pequeño procesamiento de nuestros datos con `dplyr` y `sjlabelled`, todo de una vez mediante el uso de pipes `%>%`. Para recordar los pasos para el procesamiento de datos, revisar el [curso anterior](https://descriptiva-facso.netlify.app/assignment/03-practico).

```{r proc, echo=TRUE, message=FALSE, warning=FALSE}
# Procesemos la bbdd quedandonos solo con algunas variables de interés
proc_elsoc <- elsoc_2021 %>%
    dplyr::select(
        idencuesta,
        ing_per = m13, 
        ing_per_just = m15,
        edad = m0_edad, 
        mesfuerzo = c18_09, 
        mtalento = c18_10,
        ess= d01_01
    ) %>%  # seleccionamos
    dplyr::mutate(
        across(ing_per:ess, ~ if_else(. %in% c(-666, -777, -888, -999), NA, .)), # Recodificar a NA
        pmerit = (mesfuerzo + mtalento) / 2 # Crear nueva variable
    ) %>% # recodificamos y transformamos
    labelled::set_variable_labels(
        pmerit = "Promedio entre percepción de meritocracia por esfuerzo y talento"
    ) # etiquetamos

```

Veamos cómo quedó nuestra base procesada `proc_elsoc`

```{r head, echo=TRUE}

head(proc_elsoc)

```


# 2. Correlación de Pearson

El coeficiente de correlación de Pearson es una herramienta poderosa en investigación social, ya que nos permite cuantificar la relación entre dos fenómenos. Consideremos la siguiente pregunta: _¿Cómo se relacionan los ingresos que las personas reciben con los ingresos que creen que deberían recibir?_

A partir de esta pregunta, podemos formular una hipótesis que guíe nuestra investigación: las personas con mayores ingresos podrían justificar en mayor medida recibir estos ingresos, en función de su experiencia o habilidades. Por tanto, podríamos esperar una relación positiva entre los ingresos reales y los ingresos percibidos como merecidos. Dicho de otra forma:

> A medida que aumentan los ingresos reales de las personas, también aumentan los ingresos que creen merecer.

Para poner a prueba esta hipótesis, utilizaremos las variables `ing_per` (ingresos) e `ing_per_just` (ingresos considerados justos) de la base de datos de ELSOC que previamente procesamos (`proc_elsoc`) 

En R, el coeficiente de correlación de Pearson se obtiene con la función `cor()` :

```{r ex0_cor, warning=FALSE}
cor(x = proc_elsoc$ing_per, 
    y = proc_elsoc$ing_per_just, 
    use = "complete.obs")
```

```{r, include=FALSE, warning=FALSE}
r_coef <- cor(x = proc_elsoc$ing_per, 
              y = proc_elsoc$ing_per_just, 
              use = "complete.obs")
r_coef <- round(r_coef, 2)
```

Tenemos que la correlación entre los ingresos de las personas (`ing_per`) y los ingresos que consideran merecer (`ing_per_just`) es de `r r_coef`. 

# 3. Tamaños de efecto

¿Y cómo puedo saber si el valor de la correlación es alto, medio o bajo? Si bien la correlación no nos indica causalidad, si nos permite conocer la dirección y fuerza de asociación entre dos variables. Un estándar para determinar qué tan fuerte es dicha asociación en las ciencias sociales es el propuesto por Cohen (1988).

<style>
  table {
    margin-left: auto; /* Ajustamos el margen izquierdo a automático */
    margin-right: auto; /* Ajustamos el margen derecho a automático */
    border-collapse: collapse;
    width: 60%;
    border: 2px solid black;
  }
  
  th, td {
    border: 1px solid #D3D3D3;
    padding: 8px;
    text-align: center;
  }
</style>

<table>
  <tr>
    <th class="cell-left">r</th>
    <th class="cell-left">Significado aproximado (Cohen 1988)</th>
  </tr>
  <tr>
    <td class="cell-left">&lt; ±0.1&emsp;</td>
    <td class="cell-left">Muy pequeño</td>
  </tr>
  <tr>
    <td class="cell-left">±0.1–0.3</td>
    <td class="cell-left">Pequeño</td>
  </tr>
  <tr>
    <td class="cell-left">±0.3–0.5</td>
    <td class="cell-left">Moderado</td>
  </tr>
  <tr>
    <td class="cell-left">&gt;±0.5</td>
    <td class="cell-left">Grande</td>
  </tr>
</table>

Con estos criterios podemos interpretar de mejor manera nuestros resultados de correlación. Como se observa, mientras más alto (sea en + o -) el coeficiente, más juntos estarán los datos (puntos), mostrando un patrón. 

```{r correlation-grid, echo=FALSE, out.width="80%", fig.align='center'}
make_correlated_data <- function(r, n = 200) {
    MASS::mvrnorm(
        n = n,
        mu = c(0, 0),
        Sigma = matrix(c(1, r, r, 1), nrow = 2),
        empirical = TRUE
    ) %>%
        magrittr::set_colnames(c("x", "y")) %>%
        as_tibble()
}

cor_grid <- tibble(r = c(0.2, 0.4, 0.7, 0.9)) %>%
    mutate(data = map(r, make_correlated_data)) %>%
    unnest(data)

ggplot(cor_grid, aes(x = x, y = y)) +
    geom_point(size = 2, color = "white", fill = "black", pch = 21) +
    facet_wrap(vars(r), labeller = label_both) +
    # theme_minimal() +
    theme(strip.text = element_text(face = "bold", size = rel(1.3), hjust = 0))
```

:::::: {.row .d-flex .justify-content-center}
::::: {.col-md-6} 
:::: {.card .bg-danger .text-white}
::: {.card-body}

**Interpretación**

Recordemos nuestro resultado al comienzo:

Tenemos que la correlación entre la variable de ingresos reales y los ingresos considerados justos es de `r r_coef`. ¿Cómo interpreto esto?

Una manera recomendable es la siguiente:

_El coeficiente de correlación de Pearson entre los ingresos reales y los ingresos considerados justos es **positivo y moderado** (r = `r r_coef`) según Cohen (1988)._ 

:::
::::
:::::
::::::

# 4. Diagramas de dispersión

Siempre es recomendable acompañar el valor de la correlación con una exploración gráfica de la distribución bivariada de los datos. El **gráfico o diagrama de dispersión** es una buena herramienta, ya que muestra la forma, la dirección y la fuerza de la relación entre dos variables cuantitativas.

```{r, echo=TRUE, warning=FALSE}
sjPlot::plot_scatter(data = proc_elsoc, 
                     x = ing_per,
                     y = ing_per_just)

cor(x = proc_elsoc$ing_per, 
    y = proc_elsoc$ing_per_just, 
    use = "complete.obs")
```

```{r, echo=TRUE, warning=FALSE}

maximo <- max(proc_elsoc$ing_per_just, na.rm = T)

proc_elsoc2 <- proc_elsoc %>% 
    dplyr::filter(!ing_per_just %in% maximo)

sjPlot::plot_scatter(data = proc_elsoc2, 
                     x = ing_per, 
                     y = ing_per_just)

cor(x = proc_elsoc2$ing_per, 
    y = proc_elsoc2$ing_per_just, 
    use = "complete.obs")
```

A raíz del gráfico observamos que: 

- existe un patrón positivo en los datos, en tanto los puntos están juntos y muestran una tendencia ascendente
- existen datos que se alejan del patrón y se podrían considerar extremos

::: callout-tip
#### Recurso
En el siguiente [enlace](https://rpsychologist.com/correlation/) pueden visualizar la correlación para dos variables cambiando la fuerza y el sentido de esta, al mismo tiempo que les permite observar la varianza compartida entre ambas variables.
:::

# 5. Inferencia en correlación

En el contexto de la inferencia, **la correlación** nos permite **determinar si existe (o no) una asociación estadísticamente significativa** entre dos variables. En ese sentido, la lógica del contraste de hipótesis usando correlación es:

::: {.callout-note}
#### Hipótesis en correlación

Contrastamos la _hipótesis nula_ (o de trabajo) de _no_ asociación entre variables:
$$  H_{0}: \rho = 0 $$

En relación a una _hipótesis alternativa_ sobre la existencia una asociación significativa entre variables:

$$  H_{A}: \rho \neq 0 $$
:::

Retomemos nuestro ejemplo anterior a partir de la pregunta de investigación: _¿Cómo se relacionan los ingresos que las personas reciben con los ingresos que creen que deberían recibir?_ 

Formulemos nuestra hipótesis de manera formal:

- $H_{0}$: $cor(ingresos, ingresos_{justos})$ $=$ $0$
- $H_{A}$: $cor(ingresos, ingresos_{justos})$ $\neq$ $0$

Volvamos a calcular el coeficiente para estas dos variables. Anteriormente utilizamos la función `cor()` que nos entrega la magnitud del coeficiente, pero no nos entrega información respecto a su significancia estadística. Para ello, utilizaremos la función `cor.test()`.

```{r, echo=TRUE}

cor_results <- cor.test(x = proc_elsoc$ing_per, 
                        y = proc_elsoc$ing_per_just,
                        method = "pearson",
                        use = "complete.obs") # Considerar solo datos completos (listwise)

cor_results
```

Tenemos nuestro resultado, pero es poco amigable visualmente. Generemos una tabla para nuestra correlación.

```{r, echo=TRUE, warning=FALSE}
stats.table <- tidy(cor_results)

stats.table %>%
    dplyr::mutate(
        estimate = round(estimate, 2),
        statistic = round(statistic, 2),
        ic_95 = paste0("[", round(conf.low, 2), ",", round(conf.high, 2), "]"),
        stars = gtools::stars.pval(p.value),
        p_value = case_when(
            p.value < 0.05 & p.value > 0.01 ~ "< 0.05",
            p.value < 0.01 & p.value > 0.001 ~ "< 0.01",
            p.value < 0.001 ~ "< 0.001",
            TRUE ~ ""
        ),
        p_value = paste0(p_value, stars)
    ) %>%
    dplyr::select(estimate, statistic, p_value, parameter, method, alternative, ic_95) %>%
    kableExtra::kable(
        col.names = c("Estimación", "t", "p-value", "df", "Método", "Alternativa", "95% IC"),
        booktabs = T
    ) %>%
    kableExtra::kable_styling(
        bootstrap_options = c("striped", "hover", "condensed", "responsive"),
        full_width = T,
        latex_options = "hold_position",
        position = "center"
    )
```

Ya sabemos interpretar una correlación, ahora usemos este resultado para probar nuestra hipótesis: 

> La asociación entre los ingresos reales y los ingresos justos es positiva, moderada y estadísticamente significativa (r = 0.47, p < .001). Por tanto, con un 95% de confianza se puede rechazar la $H_{0}$ de no asociación entre variables, existiendo evidencia a favor de la $H_{A}$ sobre una asociación significativa entre los ingresos reales y los ingresos justos.

# 6. Coeficiente de determinación ($R^2$)

El coeficiente de determinación $R^2$ es una medida estadística que indica la proporción de la varianza total de una variable que es explicada por otra(s) variable(s). En pocas palabras, 

- se utiliza para evaluar cuánta de la variabilidad de una variable se debe a otra variable. 
- sus valores van desde 0 a 1, en donde 0 indica que ambas variables comparten el 0% de su varianza, y 1 que comparten el 100% de su varianza. 

En el contexto de la correlación entre **solo dos variables**, el $R^2$ es igual a elevar al cuadrado el coeficiente de correlación = `(r)^2`. Esto nos permite conocer qué tanto la variabilidad de una variable X estaría asociado a la variabilidad de otra variable Y. 

En nuestro ejemplo anterior entre los ingresos reales y los ingresos justos, teníamos que su coeficiente de correlación era $r = 0.47$. Por lo tanto, creamos un efecto que incluya el resultado de esta correlación.

```{r, echo=TRUE, warning=FALSE, message=FALSE}

coef_r <- cor_results$estimate

coef_r
```

Calculemos el $R^2$ de esta asociación.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
coef_r^2
```

Con esto, podemos decir que el 21.7% de la variabilidad del ingreso real es compartido con la variabilidad de los ingresos justos.

# Ejercicio autónomo

1- A partir de la base de datos de `proc_elsoc` responda la siguiente pregunta _¿en qué medida la edad de las personas está asociada a sus percepciones sobre la meritocracia?_ Para responder esta pregunta siga los siguientes pasos:

a. Calcule el coeficiente de correlación de Pearson ($r$) entre las variables `edad` y `pmerit`. `pmerit` es una variable que promedia las variables de `mesfuerzo` y `mtalento`
b. Interprete el tamaño de efecto del coeficiente siguiendo los criterios de Cohen (1988)
c. Reporte el sentido de la dirección de la correlación
d. Interprete la significancia estadística del coeficiente
e. Visualice la relación entre las variables con un gráfico de dispersión y comente

2- Al igual que en el ejercicio anterior responde la siguiente pregunta _¿en qué medida el estatus social subjetivo de las personas está asociada a su nivel de ingresos?_ Para responder esta pregunta siga los siguientes pasos:

a. Calcule el coeficiente de correlación de Pearson ($r$) entre las variables `ess` e `ing_per`. `ess` es una variable que pregunta, en una escala de 0 a 10, en qué lugar se ubica cada encuestad-.
b. Interprete el tamaño de efecto del coeficiente siguiendo los criterios de Cohen (1988), reporte el sentido de la dirección de la correlación e interprete la significancia estadística del coeficiente
c. Visualice la relación entre las variables con un gráfico de dispersión y comente.
d. ¿Qué conclusiones se pueden obtener a partir de esta correlación entre dos variables que -de cierta forma- abordan el estatus de las personas?

# Resolución ejercicios

Formulemos nuestra hipótesis de manera formal:

- $H_{0}$: $cor(edad, pmerit)$ $=$ $0$
- $H_{A}$: $cor(edad, pmerit)$ $\neq$ $0$

Para contestar los puntos 1,2,3 y 4 usaremos el comando `cor.test` para evaluar la correlación entre `edad` y `pmerit`.

```{r re1, echo=TRUE}

results_ej <- cor.test(x = proc_elsoc$edad, y = proc_elsoc$pmerit, method = "pearson", use = "complete.obs")

results_ej

```

Con este output podemos sostener lo siguiente:

> El coeficiente de correlación de Pearson entre la edad y el promedio de meritocracia es positivo y estadísticamente significativo ($r$ = 0.11, $p$ < 0.001). Por tanto, con un 95% de confianza se puede rechazar la $H_{0}$ de no asociación entre variables, existiendo evidencia a favor de la $H_{A}$ sobre una asociación significativa entre la edad de las personas y su percepción de meritocracia. 

> Respecto al tamaño de este efecto, de acuerdo a los criterios de Cohen corresponde a un tamaño pequeño. Consistentemente, la varianza compartida ($r²$) entre ambas variables es muy baja, correspondiendo a un 0.01%.


Para contestar el punto 5 usaremos el comando `plot_scatter` para visualizar la asociación entre `edad` y `pmerit`.

```{r re2, echo=TRUE, results='asis'}
#| label: fig-ej
#| fig-cap: Scatterplot entre edad y percepción de meritocracia promedio
sjPlot::plot_scatter(data = proc_elsoc, 
                     x = edad, 
                     y = pmerit)
```

De acuerdo con la @fig-ej, podemos observar que no existe un claro patrón de asociación entre la `edad` y `pmerit`. En detalle, la nube de puntos muestra una distribución bivariada muy dispersa, por lo que es posible sostener que no hay una asociación lineal suficientemente fuerte. Esto, se puede respaldar en el coeficiente de correlación que obtuvimos antes $r$ = 0.11, ya que si bien es positivo es de un tamaño de efecto muy pequeño según los criteriores de Cohen (1988).

2.

```{r re3, echo=TRUE}

results_ej <- cor.test(x = proc_elsoc$ess, y = proc_elsoc$ing_per, method = "pearson", use = "complete.obs")

results_ej

```

Con este output podemos sostener lo siguiente:

> El coeficiente de correlación de Pearson entre el estatus social subjetivo y los ingresos es positivo y estadísticamente significativo ($r$ = 0.24, $p$ < 0.001). Por tanto, con un 95% de confianza se puede rechazar la $H_{0}$ de no asociación entre variables, existiendo evidencia a favor de la $H_{A}$ sobre una asociación significativa entre el estatus social subjetivo de las personas y sus ingresos. 

> Respecto al tamaño de este efecto, de acuerdo a los criterios de Cohen corresponde a un tamaño pequeño. Consistentemente, la varianza compartida ($r²$) entre ambas variables es muy baja, correspondiendo a un 0.06%.

```{r re4, echo=TRUE, results='asis'}
#| label: fig-ej2
#| fig-cap: Scatterplot entre estatus social subjetivo e ingresos reales
sjPlot::plot_scatter(data = proc_elsoc, 
                     x = ess, 
                     y = ing_per)
```

¿a qué se debe esta diferencia entre el estatus social subjetivo de una persona y su nivel de ingresos reales?


:::::: {.row .d-flex .justify-content-center}
::::: {.col-md-6} 
:::: {.card .bg-danger .text-white}
::: {.card-body}

**A tener en cuenta en la interpretación**

- Los dos aspectos principales en la interpretación de una prueba estadística son la **inferencia** y el **tamaño de efecto**

- En la **inferencia** se comenta si existe evidencia significativa para rechazar la hipótesis nula de no asociación. Para ello se considera:
  - el nombre de la prueba utilizada (Z, T, Pearson,Spearman, Chi2, etc), 
  - el resultado del cálculo de esta prueba empírica con los datos (ej: t=2.60)
  - información adicional si corresponde en el cálculo (ej: grados de libertad)
  - el valor p correspondiente a la prueba empírica: si bien existe un nivel convencional de rechazo de Ho p<0.05, en caso que el p sea menor que otros valores convencionales (p<0.01 o p<0.001) hay que mencionar los niveles con menor probabilidad de error.

- En el caso de **tamaño de efecto** hay que considerar aquellos específicos de cada prueba, por ejemplo criterios de Cohen y R2 para Pearson, o Phi/Cramer para Chi 2.

:::
::::
:::::
::::::