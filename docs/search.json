[
  {
    "objectID": "assignment/08-practico.html",
    "href": "assignment/08-practico.html",
    "title": "Inferencia 4: Pruebas de hipótesis II",
    "section": "",
    "text": "El objetivo de esta guía práctica es profundizar en la inferencia estadóstica, particularmente en el contraste de hipótesis de diferencias entre dos grupos.\nEn detalle, aprenderemos:\n\nAplicar pruebas de hipótesis direccionales.\nAplicar inferencia estadística a proporciones.\nEmplear la correlación en contexto de inferencia.\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022.\n\n\n\nEn inferencia, las pruebas de hipótesis nos ayudan a determinar si el resultado que obtenemos en nuestra muestra es un efecto real/extensible a la población o un error de muestreo. Aquí recomendamos una lista de cinco pasos lógicos para enfrentarnos a la inferencia estadística:\n\n\n\n\nPaso\n\n\nDetalle\n\n\n\n\n1\n\n\nFormula \\(H_0\\) y \\(H_A\\) y estipula la dirección de la prueba\n\n\n\n\n2\n\n\nCalcula el error estándar (SE)\n\n\n\n\n3\n\n\nCalcula el valor estimado de la prueba (ej: Z o t)\n\n\n\n\n4\n\n\nEspecifica el valor crítico de la prueba\n\n\n\n\n5\n\n\nContrasta el valor estimado con el valor crítico e intrepreta los resultados\n\n\n\nAdemás de estos 5 pasos también existe la posibilidad de calcular un intervalo de confianza, que acompañe la precisión de nuestra estimación."
  },
  {
    "objectID": "assignment/08-practico.html#recursos-de-la-práctica",
    "href": "assignment/08-practico.html#recursos-de-la-práctica",
    "title": "Inferencia 4: Pruebas de hipótesis II",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022."
  },
  {
    "objectID": "assignment/08-practico.html#cinco-pasos-para-la-inferencia-estadística",
    "href": "assignment/08-practico.html#cinco-pasos-para-la-inferencia-estadística",
    "title": "Inferencia 4: Pruebas de hipótesis II",
    "section": "",
    "text": "En inferencia, las pruebas de hipótesis nos ayudan a determinar si el resultado que obtenemos en nuestra muestra es un efecto real/extensible a la población o un error de muestreo. Aquí recomendamos una lista de cinco pasos lógicos para enfrentarnos a la inferencia estadística:\n\n\n\n\nPaso\n\n\nDetalle\n\n\n\n\n1\n\n\nFormula \\(H_0\\) y \\(H_A\\) y estipula la dirección de la prueba\n\n\n\n\n2\n\n\nCalcula el error estándar (SE)\n\n\n\n\n3\n\n\nCalcula el valor estimado de la prueba (ej: Z o t)\n\n\n\n\n4\n\n\nEspecifica el valor crítico de la prueba\n\n\n\n\n5\n\n\nContrasta el valor estimado con el valor crítico e intrepreta los resultados\n\n\n\nAdemás de estos 5 pasos también existe la posibilidad de calcular un intervalo de confianza, que acompañe la precisión de nuestra estimación."
  },
  {
    "objectID": "assignment/03-practico.html",
    "href": "assignment/03-practico.html",
    "title": "Matrices de correlación y tamaños de efecto",
    "section": "",
    "text": "El objetivo de esta guía práctica es conocer maneras de reportar coeficientes de correlación y cómo interpretar sus tamaños de efecto en ciencias sociales. Además, nos introduciremos en el tratamiento de valores perdidos y otras medidas de correlación entre variables.\nEn detalle, aprenderemos:\n\nCómo reportar y presentar matrices de correlación.\nInterpretar el tamaño de efecto de una correlación.\nTratamiento de casos perdidos.\nQué es y cómo calcular la correlación de Spearman.\nQué es el coeficiente de determinación \\(R^2\\).\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué era la correlación?\nLa correlación es una medida de asociación entre variables, que describe el sentido (dirección) y fuerza de la asociación.\nEn otras palabras, nos permite conocer cómo y cuánto se relaciona la variación de una variable, con la variación de otra variable.\n\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016."
  },
  {
    "objectID": "assignment/03-practico.html#recursos-de-la-práctica",
    "href": "assignment/03-practico.html#recursos-de-la-práctica",
    "title": "Matrices de correlación y tamaños de efecto",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016."
  },
  {
    "objectID": "assignment/03-practico.html#tratamiento-de-casos-perdidos",
    "href": "assignment/03-practico.html#tratamiento-de-casos-perdidos",
    "title": "Matrices de correlación y tamaños de efecto",
    "section": "Tratamiento de casos perdidos",
    "text": "Tratamiento de casos perdidos\nTrabajar con datos a menudo implica enfrentar valores perdidos (NA), lo que puede ser un gran desafío. Estos valores indican la ausencia de un valor en una base de datos. Los valores perdidos pueden originarse por diversas razones, como el sesgo de no respuesta en encuestas, errores en la entrada de datos o simplemente la falta de información para ciertas variables.\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\nNA\n4\n1\nHola\n\n\n7\n1\n4\nNo soy un NA\n\n\n8\nNA\n2\nNA\n\n\n9\nNA\n9\nAmo R\n\n\n3\n3\n6\nNA\n\n\n\n\n\n\n\n\nLa presencia de valores perdidos puede tener un impacto considerable en la precisión y confiabilidad de los análisis estadísticos, lo que a su vez puede conducir a resultados sesgados y conclusiones incorrectas.\nExisten varias formas de tratar valores perdidos, que van desde enfoques simples hasta métodos más complejos, como la imputación. En esta ocasión, nos centraremos en las dos estrategias más comunes:\n\ntrabajar exclusivamente con casos completos (listwise) o\nretener los casos con valores perdidos, pero excluyéndolos al calcular estadísticas (pairwise).\n\n\na) Analísis con casos completos: listwise deletion\nEste enfoque es uno de los más conocidos: implica remover completamente las observaciones que tienen valores perdidos en cualquier variable de interés. En otras palabras, si una fila/caso en un conjunto de datos tiene al menos un valor faltante en alguna de las variables que estás considerando, se eliminará por completo.\nEn R, esto podemos hacerlo con la función na.omit. Para hacer esto, sigamos estos pasos:\n\nrespaldar la base de datos original en el espacio de trabajo (por si queremos en adelante realizar algún análisis referido a casos perdidos)-\ncontamos el número de casos con el comando dim.\ncontamos cuántos y en dónde tenemos casos perdidos.\nborramos los casos perdidos con na.omit.\ncontamos nuevamente con dim para asegurarnos que se borraron.\n\n\nproc_elsoc_original &lt;- proc_elsoc\ndim(proc_elsoc)\n\n[1] 2927    7\n\n\n\nsum(is.na(proc_elsoc))\n\n[1] 81\n\n\n\ncolSums(is.na(proc_elsoc))\n\nmesfuerzo  mtalento       ess    edcine      sexo      edad    pmerit \n       18        20        12         2         0         0        29 \n\n\n\nproc_elsoc &lt;- na.omit(proc_elsoc)\ndim(proc_elsoc)\n\n[1] 2887    7\n\n\nAhora nos quedamos con 2887 observaciones sin casos perdidos.\nAunque simple de implementar, con este enfoque podemos perder información importante, especialmente si los valores perdidos no se distribuyen aleatoriamente.\n\nSiempre hay que intentar rescatar la mayor cantidad de casos posibles. Por lo tanto, si un listwise genera más de un 10% de casos perdidos se debe detectar qué variables esta produciendo esta pérdida e intentar recuperar datos. Puedes revisar un ejemplo aquí.\n\n\n\nb) Retener pero excluir: pairwise deletion\nA diferencia del anterior, este es un enfoque en el que las observaciones se utilizan para el análisis siempre que tengan datos disponibles para las variables específicas que se están analizando. En lugar de eliminar toda una fila si falta un valor, se eliminan solo los valores faltantes en las variables que se están analizando en ese momento.\nPara hacer esto en R debemos siempre verificar e indicar en nuestro código si queremos (o no) remover los NA para realizar los análisis.\n\nmean(proc_elsoc_original$pmerit); mean(proc_elsoc$edad); mean(proc_elsoc$ess)\n\n[1] NA\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\nmean(proc_elsoc_original$pmerit, na.rm = TRUE); mean(proc_elsoc$edad, na.rm = TRUE); mean(proc_elsoc$ess, na.rm = TRUE)\n\n[1] 2.653899\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\n\nCon el primer código no obtuvimos información sustantiva en ciertas variables, pero con el segundo sí al remover los NA solo de dicha variable para un cálculo determinado."
  },
  {
    "objectID": "assignment/10-practico.html",
    "href": "assignment/10-practico.html",
    "title": "Asociación con categóricas",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en técnicas de asociación entre variables categóricas, aplicando lo apredendido hasta ahora sobre inferencia estadística.\nEn detalle, aprenderemos:\n\nGenerar y analizar tablas de contingencia (o cruzadas)\nEstimar e interpretar la prueba de Chi-cuadrado\nAplicar coeficientes de correlación entre variables categóricas\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022."
  },
  {
    "objectID": "assignment/10-practico.html#recursos-de-la-práctica",
    "href": "assignment/10-practico.html#recursos-de-la-práctica",
    "title": "Asociación con categóricas",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados de la Encuesta de Caracterización Socioeconómica (CASEN) del año 2022, elaborada por el Ministerio de Desarrollo Social y Familia. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  CASEN 20222. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos CASEN 2022."
  },
  {
    "objectID": "assignment/11-practico.html",
    "href": "assignment/11-practico.html",
    "title": "Bonus track: Trabajando con CASEN y EBS",
    "section": "",
    "text": "En este guía práctica se presentan recomendaciones para trabajar de manera más eficiente con la Encuesta CASEN 2022. Además, se presenta el código para poder vincular la Encuesta de Bienestar Social (EBS) con la Encuesta CASEN en Pandemia 2020 para aquellos grupos que quieran utilizar ambas fuentes de información."
  },
  {
    "objectID": "assignment/11-practico.html#paso-1.-descargar-ambas-bases",
    "href": "assignment/11-practico.html#paso-1.-descargar-ambas-bases",
    "title": "Bonus track: Trabajando con CASEN y EBS",
    "section": "Paso 1. Descargar ambas bases",
    "text": "Paso 1. Descargar ambas bases\nEste paso ya lo hicimos antes, solo que ahora debemos descargar dos bases de datos distintas. La documentación asociada a estas bases de datos y la descripción de las variables que contienen, se encuentra disponible en el Libro de Códigos elaborados para dicha finalidad, el que puede ser descargado conjuntamente con la bases de datos.\nIngresamos a la página del Observatorio Social del MDSF y en la pestaña de “Encuestas” elegimos Encuesta de Bienestar y CASEN en Pandemia, respectivamente. Luego, en la pestaña “Bases de datos” descargamos la base en alguno de los dos formatos disponibles (.sav de SPSS o .dta de Stata). También se recomienda descargar el Libro de Códigos respectivo y manuales metodológicos asociados.\nAmbas bases de datos las alojamos en la carpeta local input &gt; datos de nuestro proyecto."
  },
  {
    "objectID": "assignment/11-practico.html#paso-2.-procesamiento-en-r-1",
    "href": "assignment/11-practico.html#paso-2.-procesamiento-en-r-1",
    "title": "Bonus track: Trabajando con CASEN y EBS",
    "section": "Paso 2. Procesamiento en R",
    "text": "Paso 2. Procesamiento en R\nAl igual que con CASEN 2022, la CASEN en Pandemia 2020 es una base de datos pesada, por lo que se recomienda realizar primero el paso 2 de la sección anterior, seleccionando las variables de interés y filtrando los casos que correspondan de ser el caso. Recuerda que, una vez terminado este paso, borra la CASEN en Pandemia 2020 original para ahorrar memoria en nuestro ordenador.\nComencemos por preparar nuestros datos. Iniciamos cargando las librerías necesarias.\n\npacman::p_load(tidyverse, # Manipulacion datos\n               haven, # Importar datos\n               sjmisc, # Descriptivos\n               sjlabelled) # Etiquetas\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\nLuego, importamos nuestra CASEN 2020 recortada y la EBS 2021.\n\nload(file = \"output/datos/casen2020_recorte.RData\")\nebs &lt;- haven::read_dta(file = \"input/datos/Base de datos EBS 2021 STATA.dta\")\n\nComo mencionamos antes, debemos renombrar las variables folio e id_persona de la base CASEN 2020 recortada para poder unirla con la EBS 2021.\n\ncasen &lt;- casen %&gt;%\n  rename(folio_casen = folio,\n         id_persona_casen = id_persona)\n\nnames(casen) # verificamos\n\n [1] \"folio_casen\"      \"id_persona_casen\" \"id_vivienda\"      \"region\"          \n [5] \"hogar\"            \"expr\"             \"expp\"             \"expc\"            \n [9] \"varstrat\"         \"varunit\"          \"sexo\"             \"y1\"              \n\n\nPara unir ambas bases de datos, usaremos la función left_join() del paquete dplyr. En esta función, ponemos a la izquierda la base de datos a la que queremos que se le agrege información y a la derecha la base de datos desde la cual proviene la información. Además, debemos proveer un argumento que indica el nombre de las variables que servivarán como llave (es decir, que están presentes en ambas bases).\n\ndatos_proc &lt;- left_join(ebs, casen, by = c(\"folio_casen\", \"id_persona_casen\")) # unimos\n\ndatos_proc %&gt;% \n  head() # verificamos\n\n# A tibble: 6 × 263\n  folio  folio_casen id_persona_casen region.x      provincia comuna     zona   \n  &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl+lbl&gt;     &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;  &lt;dbl+l&gt;\n1 10002 110110010301              118 1 [1 Región … 11 [11 I… 1101 [Iqu… 1 [Urb…\n2 10003 110110020301              409 1 [1 Región … 11 [11 I… 1101 [Iqu… 1 [Urb…\n3 10004 110110030201              454 1 [1 Región … 11 [11 I… 1101 [Iqu… 1 [Urb…\n4 10008 110110031801             1050 1 [1 Región … 11 [11 I… 1101 [Iqu… 1 [Urb…\n5 10009 110110040501             1121 1 [1 Región … 11 [11 I… 1101 [Iqu… 1 [Urb…\n6 10013 110110060201             1510 1 [1 Región … 11 [11 I… 1101 [Iqu… 1 [Urb…\n# ℹ 256 more variables: sexo.x &lt;dbl+lbl&gt;, fexp &lt;dbl&gt;, p1 &lt;dbl+lbl&gt;,\n#   p2 &lt;dbl+lbl&gt;, p3 &lt;dbl+lbl&gt;, p4 &lt;dbl+lbl&gt;, p11 &lt;dbl+lbl&gt;, p12 &lt;dbl+lbl&gt;,\n#   l0 &lt;dbl&gt;, l1 &lt;dbl&gt;, l2 &lt;dbl+lbl&gt;, l3 &lt;dbl&gt;, l4_1 &lt;dbl+lbl&gt;, l4_2 &lt;dbl+lbl&gt;,\n#   l4_3 &lt;dbl+lbl&gt;, l4_4 &lt;dbl+lbl&gt;, l4_5 &lt;dbl+lbl&gt;, l4_6 &lt;dbl+lbl&gt;,\n#   l5 &lt;dbl+lbl&gt;, l6 &lt;dbl+lbl&gt;, l7 &lt;dbl+lbl&gt;, l8 &lt;dbl+lbl&gt;, l9 &lt;dbl+lbl&gt;,\n#   l10a &lt;dbl+lbl&gt;, l10b &lt;dbl+lbl&gt;, l11 &lt;dbl+lbl&gt;, l12 &lt;dbl+lbl&gt;,\n#   tramoebs1 &lt;dbl+lbl&gt;, disc &lt;dbl+lbl&gt;, neduc_ebs &lt;dbl+lbl&gt;, a1 &lt;dbl+lbl&gt;, …\n\n\nPor último, verificamos que se mantenga la cantidad de casos original de la EBS como mencionamos antes.\n\ndim(datos_proc) \n\n[1] 10921   263\n\n\nMantenemos 10.921 personas.\nAhora que ya tenemos una base final con información de la EBS y CASEN 2020, podemos continuar con los demás procesamientos necesarios para nuestra investigación."
  },
  {
    "objectID": "assignment/06-practico.html",
    "href": "assignment/06-practico.html",
    "title": "Inferencia 2: Intervalos de confianza",
    "section": "",
    "text": "El objetivo de esta guía práctica es continuar profundizando en la inferencia estadística, revisando algunos de sus conceptos clave como distribución muestral, error estándar, y la definición y aplicaciones de los intervalos de confianza.\nEn detalle, aprenderemos:\n\nQué es una distribución muestral.\nQué es el error estándar.\nQué son y cómo calcular intervalos de conafianza."
  },
  {
    "objectID": "assignment/06-practico.html#qué-es-la-inferencia-estadística",
    "href": "assignment/06-practico.html#qué-es-la-inferencia-estadística",
    "title": "Inferencia 2: Intervalos de confianza",
    "section": "¿Qué es la inferencia estadística?",
    "text": "¿Qué es la inferencia estadística?\nLa inferencia estadística es el proceso de realizar conclusiones o predicciones sobre una población a partir de una muestra o subconjunto representativo de esa población. En concreto, llamamos inferencia al ejercicio de extrapolar determinados valores de una muestra, llamados estadísticos, para estimar un parámetro de la población.\n\nUn concepto central en todo esto es la probabilidad de error, es decir, en qué medida nos estamos equivocando (o estamos dispuestos a estar equivocados) al tratar de extrapolar una estimación muestral a la población."
  },
  {
    "objectID": "assignment/06-practico.html#qué-es-una-distribución",
    "href": "assignment/06-practico.html#qué-es-una-distribución",
    "title": "Inferencia 2: Intervalos de confianza",
    "section": "¿Qué es una distribución?",
    "text": "¿Qué es una distribución?\nRecordemos que por distribución nos referimos al conjunto de todos los valores posibles de una variable y las frecuencias (o probabilidades) con las que se producen.\nExisten distribuciones empíricas y distribuciones teóricas, en donde:\n\nlas primeras reflejan la distribución de los valores que asume la variable en un grupo concreto a partir de una observación.\nlas segundas son una función matématica que expresan la distribución de un conjunto de números mediante su probabilidad de ocurencia.\n\nComo vimos en el práctico 4, una de las distribuciones teóricas más conocidas es la distribución normal estándar."
  },
  {
    "objectID": "assignment/06-practico.html#cálculo-de-intervalos-de-confianza",
    "href": "assignment/06-practico.html#cálculo-de-intervalos-de-confianza",
    "title": "Inferencia 2: Intervalos de confianza",
    "section": "Cálculo de intervalos de confianza",
    "text": "Cálculo de intervalos de confianza\nAhora ¡Manos a la obra!\nCalculemos intervalos de confianza. Primero, carguemos las librerías necesarias:\n\nlibrary(pacman)\npacman::p_load(tidyverse, # colección de paquetes para manipulación de datos\n               car,       # para recodificar\n               psych,     # para analizar datos\n               sjmisc,    # para analizar datos\n               srvyr,     # para estimación de IC y ponderadores\n               Publish)   # para IC\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\ny también carguemos la base de datos que utilizaremos, que corresponde a un subset de la Encuesta Suplementaria de ingresos ESI para ocupados:\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/esi-2021-ocupados.rdata\"))\n\n\n\n\n\n\n\nNota\n\n\n\nRecordemos que podemos contar con bases de datos que tengan factor de expansión (ponderador) o no. Esta distinción se presenta cuando trabajamos con muestras simples o complejas. Al trabajar con muestras complejas debemos identificar cuál es la variable del ponderador e incorporarla en nuestro cálculo, como veremos a continuación.\n\n\n\nIntervalos de confianza sin ponderador\nPodemos calcular intervalos de confianza con muestras representativas sin ponderadores o factores de expansión. Supongamos que es el caso.\n\nIC para Medias\nCalculemos un intervalo de confianza para la media de ingresos de personas ocupadas:\n\npsych::describe(esi$ing_t_p)\n\n   vars     n     mean       sd   median  trimmed      mad min      max\nX1    1 37124 586360.4 697362.9 405347.7 474473.1 255411.6   0 38206253\n      range skew kurtosis      se\nX1 38206253   12   402.32 3619.36\n\n\n\nPublish::ci.mean(esi$ing_t_p, alpha = 0.05)\n\n mean      CI-95%               \n 586360.41 [579266.37;593454.45]\n\n\nAl no aplicar factores de expansión, contamos con una media de ingresos de $586.360 como estimación puntual. Pero también podemos decir que con un 95% de confianza el parámetro poblacional se encontrará entre $579.266 y $593.454.\n\n\nIC para Proporciones\nPara calcular un intervalo de confianza para la proporción por la variable sexo, usamos:\n\nsjmisc::frq(esi$sexo)\n\nx &lt;numeric&gt; \n# total N=37124 valid N=37124 mean=1.44 sd=0.50\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 20806 | 56.04 |   56.04 |  56.04\n    2 | 16318 | 43.96 |   43.96 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nprop.test(x = 20806, n = 37124, conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  20806 out of 37124, null probability 0.5\nX-squared = 542.32, df = 1, p-value &lt; 0.00000000000000022\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5553777 0.5655019\nsample estimates:\n        p \n0.5604461 \n\n\nEn este caso, sabemos que el total de las personas ocupadas de la muestra son n=37.124, y que la cantidad de hombres son 20.806, correspondientes al 56% como estimación puntual. También podemos sostener con un 95% que la proporción de hombres en la población se encuentra entre 55.54% y 56.6%.\n\n\n\nIntervalos de confianza con ponderador\nPara muestras complejas que cuentan con ponderador (o factor de expansión) también podemos hacer este ejercicio.\nPrimero, es necesario identificar la variable de factor de expansión o ponderador:\n\nesi_pond &lt;- esi %&gt;% as_survey_design(ids = 1, # indica conglomerados de muestreo; ~0 o ~1 cuando no hay\n                                     strata = estrato, # indica efecto de diseño muestral\n                                     weights = fact_cal_esi) # indica el ponderador\n\noptions(survey.lonely.psu = \"certainty\") # seteamos para que ids no moleste\n\n\nIC para Medias\nAhora, teniendo en consideración el factor de expansión, podemos señalar que:\n\nesi_pond %&gt;% \n  summarise(media = survey_mean(ing_t_p, vartype = \"ci\", levels = 0.95, na.rm=TRUE)) # usamos funcion survey_mean\n\n# A tibble: 1 × 3\n    media media_low media_upp\n    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 681039.   666563.   695516.\n\n\nEl promedio de ingresos de personas ocupadas ponderado en la población corresponde a $681.039 como estimación puntual, pero que es posible afirmar con un 95% de confianza que el parámetro poblacional se encuentra entre $666.562 y $695.516.\n\n\nIC para Proporciones\nFinalmente, si calculamos la proporción de hombres ocupados en la población considerando el factor de expansión:\n\nsjmisc::frq(esi$sexo)\n\nx &lt;numeric&gt; \n# total N=37124 valid N=37124 mean=1.44 sd=0.50\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 20806 | 56.04 |   56.04 |  56.04\n    2 | 16318 | 43.96 |   43.96 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nesi_pond %&gt;% \n  group_by(sexo) %&gt;% # agrupamos por sexo\n  summarise(prop = survey_prop(vartype = \"ci\", levels = 0.95, na.rm = TRUE))\n\n# A tibble: 2 × 4\n   sexo  prop prop_low prop_upp\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     1 0.582    0.575    0.590\n2     2 0.418    0.410    0.425\n\n\nTenemos que, con un 95% de conafianza, podemos afirmar que la proporción de hombre ocupados se encuentra entre el 57% y 58%."
  },
  {
    "objectID": "assignment/07-practico.html",
    "href": "assignment/07-practico.html",
    "title": "Inferencia 3: Pruebas de hipótesis",
    "section": "",
    "text": "Objetivo de la práctica\nEl objetivo de esta guía práctica es continuar profundizando en la inferencia estadística, en particular en contráste de hipótesis de diferencia de medias. Este tema se desarrolló en la sesión 7.\n\n\nTest de hipótesis para diferencia de medias\nPrimero, carguemos las librerías necesarias:\n\nlibrary(pacman)\npacman::p_load(tidyverse,  # colección de paquetes para manipulación de datos\n               car,        # para recodificar\n               psych,      # para analizar datos\n               sjmisc,     # para analizar datos\n              # srvyr,      # para estimación de IC y ponderadores\n              # Publish,    # para IC\n               kableExtra) # para presentación de tablas\n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo\n\nVamos a testear la siguiente hipótesis:\n\n\\(H_a\\): existen diferencias de edad entre hombres y mujeres\n\nY su correspondiente hipótesis nula:\n\n\\(H_0\\): no existen diferencias de edad entre hombres y mujeres\n\nGeneración de datos (muestra_3) y descriptivos:\n\nmuestra_3 &lt;- data.frame(edad=c(33,35,23,32,24,25,29,31,32,31),\n                        sexo=c(1,1,2,1,2,2,2,1,1,1))\n\nmean(muestra_3$edad)\n\n[1] 29.5\n\nmuestra_3$sexo &lt;- as.factor(muestra_3$sexo)\n\nmuestra_3 %&gt;% \n  group_by(sexo) %&gt;% \n  summarise(media=mean(edad)) \n\n# A tibble: 2 × 2\n  sexo  media\n  &lt;fct&gt; &lt;dbl&gt;\n1 1      32.3\n2 2      25.2\n\n\nGráfico descriptivo:\n\nmuestra_3 %&gt;% \n  group_by(sexo) %&gt;% \n  summarise(media=mean(edad)) %&gt;% \n  ggplot(aes(x=sexo, y=media)) +\n  geom_point() +\n  ylim(25,35) +\n  labs(title = \"Medias de edad para hombres y mujeres, muestra 3\",\n                   x = \"Sexo\",\n                   y = \"Media edad\")\n\n\n\n\n\n\n\n\nPrueba t de diferencia de medias:\n\nt.test(edad ~ sexo,data=muestra_3)\n\n\n    Welch Two Sample t-test\n\ndata:  edad by sexo\nt = 4.8799, df = 4.33, p-value = 0.006658\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n  3.171534 10.995133\nsample estimates:\nmean in group 1 mean in group 2 \n       32.33333        25.25000 \n\n\nLo principal en este output es el valor p, que es la probabilidad de error de rechazar la hipótesis nula. En este caso, \\(p = 0.006658\\), que es menor a un nivel de confianza convencional como \\(\\alpha = 0.05\\), incluso es menor que un nivel más exigente como el \\(\\alpha = 0.01\\). Por lo tanto, rechazamos la hipótesis nula de igualdad de medias con un 99% de confianza, hay suficiente evidencia estadística para sostener que el promedio de edad de hombres y mujeres es diferente.\n\n\nCálculo paso a paso de estadístico t\nEn esta última sección se realizará el cálculo paso a paso del estadístico \\(t\\) del ejemplo anterior para demostrar cómo se origina la información que aparece en el output de R.\nRecordemos la fórmula de t:\n\\(t=\\frac{(\\bar{x}_1-\\bar{x}_2)}{\\sqrt{\\frac{s_1²}{\\sqrt{n_1}}+\\frac{s_2²}{\\sqrt{n_2}} }}\\)\nDonde en la parte superior se encuentra la diferencia de medias entre dos grupos, y en la inferior el error estándar de t.\nPasos:\n\nSe calcula la diferencia de medias\nSe calcula el error estándar de la diferencia de medias\nCálculo del valor t\nSe fija un \\(\\alpha\\) (usualmente 0.05) para rechazar \\(H_0\\), y se busca el valor crítico asociado a este \\(\\alpha\\) (en una tabla de valores t, o en R)\nSi nuestro t es superior al valor crítico, se rechaza \\(H_0\\)\n\nPaso 1: Calculamos la diferencia de medias \\((\\bar{x}_1-\\bar{x}_2)\\)\n\nmuestra_3 %&gt;%\n   dplyr::group_by(sexo=sjlabelled::as_label(sexo)) %&gt;% # se agrupan por la variable categórica y se usan sus etiquetas con as_label\n  dplyr::summarise(Obs.=n(),Promedio=mean(edad, na.rm=TRUE),SD=sd(edad, na.rm=TRUE)) %&gt;% # se agregan las operaciones a presentar en la tabla\n  kable(format = \"markdown\")\n\n\n\n\nsexo\nObs.\nPromedio\nSD\n\n\n\n\n1\n6\n32.33333\n1.505545\n\n\n2\n4\n25.25000\n2.629956\n\n\n\n\ndif_medias &lt;- 32.333 - 25.250\ndif_medias\n\n[1] 7.083\n\n\nPaso 2: Calculamos el error estándar de la diferencia de medias: \\(\\sqrt{\\frac{s_1²}{\\sqrt{n_1}}+\\frac{s_2²}{\\sqrt{n_2}}}\\)\n\nmuestra_3h &lt;- muestra_3 %&gt;% filter(sexo==1)\nmuestra_3m &lt;- muestra_3 %&gt;% filter(sexo==2)\n  \ns_h &lt;- sd(muestra_3h$edad)\nn_h &lt;- length(muestra_3h$edad)\ns_m &lt;- sd(muestra_3m$edad)\nn_m &lt;- length(muestra_3m$edad)\n\nee &lt;- sqrt((s_h^2)/n_h + (s_m^2)/n_m)\nee\n\n[1] 1.451532\n\n\nPaso 3: Cálculo del valor t\n\nte &lt;- dif_medias/ee\nte\n\n[1] 4.879673\n\n\nPaso 4: Fijamos un \\(\\alpha\\) y se busca el valor crítico de t asociado al \\(\\alpha\\). En este caso utilizaremos el valor usual de \\(\\alpha = 0.05\\).\n\ntt &lt;- qt(0.05/2,df=9,lower.tail = F)\ntt\n\n[1] 2.262157\n\n\nPaso 5: test de hipótesis\nSegún la distribución t, el valor crítico para poder rechazar \\(H_0\\) con un 95% de confianza es 2.26. El t calculado con información de la muestra (o t empírico) es 4.87. Este valor es superior al t crítico, por lo tanto se rechaza \\(H_0\\) con un 95% de confianza, o una probabilidad de error p&lt;0.05.\n\n\nReporte de progreso\nCompletar el reporte de progreso correspondiente a esta práctica aquí. El plazo para contestarlo es hasta el día martes de la semana siguiente a la que se publica la guía práctica.\n\n\nVideo de práctica\n\n\n\nForo"
  },
  {
    "objectID": "assignment/01-practico.html",
    "href": "assignment/01-practico.html",
    "title": "Distribución Normal e Intervalos de Confianza",
    "section": "",
    "text": "El objetivo de esta guía práctica es introducirnos en la inferencia estadística, revisando los conceptos y aplicaciones de la curva normal y las probabilidades bajo esta con puntajes Z, además del cálculo de intervalos de confianza.\nEn detalle, aprenderemos y recordaremos:\n\nLos conceptos de promedio y desviación estándar\nQué es la probabilidad y su aplicación para estadística\nQué es la distribución normal\nCómo calcular e interpretar intervalos de confianza\n\n\n\nCargaremos algunas librerías que serán necesarias en las diferentes partes de esta guía práctica:\n\nlibrary(pacman)\npacman::p_load(tidyverse, # para sintaxis\n               ggplot2,   # para gráficos\n               car,       # para recodificar\n               psych,     # para analizar datos\n               sjmisc,    # para analizar datos\n               srvyr,     # para estimación de IC y ponderadores\n               Publish)   # para IC)    \n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls())       # para limpar el entonrno de trabajo"
  },
  {
    "objectID": "assignment/01-practico.html#formatos-básicos-de-markdown",
    "href": "assignment/01-practico.html#formatos-básicos-de-markdown",
    "title": "Reportes dinámicos con RMarkdown",
    "section": "Formatos básicos de Markdown",
    "text": "Formatos básicos de Markdown\n\n\n\nEscribe…\n…o…\n…para obtener\n\n\n\n\nAlgo de texto en el párrafo.\n\nMás texto\nespacio entre lineas.\n\nAlgo de texto.\nAlgo de texto en el párrafo. Siempre utilizando espacios para dividir párrafos\n\n\n`*Cursivas*`\n`_Cursivas_`\nCursivas\n\n\n`**Negrita**`\n`__Negrita__`\nNegrita\n\n\n# Título 1\n\nTítulo 1\n\n\n## Título 2\n\nTítulo 2\n\n\n### Título 3\n\nTítulo 3\n\n\n(puedes llegar hasta un título N° 6 con ######)\n\n\n\n\n`[Link text](http://www.example.com)`\n\nLink text\n\n\n`![Image caption](/path/to/image.png)`\n\n\n\n\nClass logo\n\n\n\n\n` ```Inline code``` ` with backticks\n\nInline code with backticks\n\n\n&gt; Citas\n\n\nCitas\n\n\n\n- Cosas en\n- listas\n- desordenadas\n* Cosas en\n* listas\n* desordenadas\n\nCosas en\nlistas\ndesordenadas\n\n\n\n1. Cosas en\n2. listas\n3. ordenadas\n1) Cosas en\n2) listas\n3) ordenadas\n\nCosas en\nlistas\nordenadas\n\n\n\nLínea horizontal\n\n---\nLínea horizontal\n\n***\nLínea horizontal"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Programa",
    "section": "",
    "text": "Prof. Juan Carlos Castillo\n   325 Sociología FACSO, Universidad de Chile\n   juancastillov@uchile.cl\n   Agendar reunión\n\n\n\n\n\n   Lunes y Martes\n   12 Agosto al 29 de Noviembre, 2023\n   10:15-11:45 (Lunes) y 8:30-10:00 (Martes)\n   Lunes - Aulario C6, Martes - Aulario C7-C8 & FACSO 345\n   Slack"
  },
  {
    "objectID": "syllabus.html#sobre-el-sentido-general-del-curso",
    "href": "syllabus.html#sobre-el-sentido-general-del-curso",
    "title": "Programa",
    "section": "Sobre el sentido general del curso",
    "text": "Sobre el sentido general del curso\nEn este curso vamos a aprender tres cosas principales:\n\ninferencia: los resultados que encontramos en nuestra muestra, ¿se encuentran también en la población de la cual proviene la muestra?\nmedidas de asociación entre variables: tamaño y significación estadística\nreporte y reproducibilidad de los análisis estadísticos: nuestros análisis se reflejan en productos como tablas y gráficos. No basta con entenderlos e interpretarlos, sino también es fundamental una buena comunicación."
  },
  {
    "objectID": "syllabus.html#propósito-general-del-curso",
    "href": "syllabus.html#propósito-general-del-curso",
    "title": "Programa",
    "section": "Propósito general del curso",
    "text": "Propósito general del curso\nAl finalizar el curso los estudiantes conocerán los fundamentos del análisis estadístico inferencial. Se espera que los estudiantes sean capaces de:\n\nelaborar de manera pertinente hipótesis estadísticas\naplicar estadísticos de asociación bivariada, a partir de los cuáles puedan desarrollar análisis de problemas sociales\ncorroborar el cumplimiento de las condiciones de aplicación de cada estadístico\nutilizar software de análisis estadístico\ncontrastar hipótesis de investigación\nelaborar conclusiones integrando fundamentos teóricos con herramientas de análisis estadístico de resultados.\n\nComplementariamente se espera que los estudiantes adquieran herramientas que les permitan comunicar resultados de investigación en contextos sociales, profesionales y académicos."
  },
  {
    "objectID": "syllabus.html#competencias",
    "href": "syllabus.html#competencias",
    "title": "Programa",
    "section": "Competencias",
    "text": "Competencias\n1a. Delimitar, conceptualizar y analizar diversos objetos de investigación social, con especial énfasis en aquellos relacionados con los procesos de transformación del país y Latinoamérica\n1b. Manejar diversas estrategias metodológicas de las ciencias sociales\n1c. Manejar un conjunto de herramientas para el procesamiento y análisis de información\n1d. Transmitir los conocimientos derivados de la práctica investigativa, así como aquellos adquiridos durante el proceso formativo."
  },
  {
    "objectID": "syllabus.html#subcompetencias",
    "href": "syllabus.html#subcompetencias",
    "title": "Programa",
    "section": "Subcompetencias",
    "text": "Subcompetencias\n\n1.4 Contribuir a generar conocimiento sociológico en el marco de estudios y/o procesos de investigación donde se articulen creativamente las dimensiones teórica, metodológica y práctica.\n1.5 Comunicar los saberes disciplinares de manera pertinente a las características de distintos contextos y audiencias, utilizando diversas estrategias y formatos."
  },
  {
    "objectID": "syllabus.html#resultados-del-aprendizaje",
    "href": "syllabus.html#resultados-del-aprendizaje",
    "title": "Programa",
    "section": "Resultados del aprendizaje",
    "text": "Resultados del aprendizaje\n\nComprende, domina y es capaz de explicar los elementos conceptuales subyacentes a la determinación de la asociación poblacional entre dos variables a partir del análisis de una muestra, y es capaz de traducir hipótesis derivadas de la teoría sociológica en hipótesis estadísticas posibles de contrastar empíricamente con los datos.\nEs capaz de seleccionar y usar herramientas estadísticas adecuadas para evaluar la asociación entre dos variables considerando las características de los datos y las condiciones de aplicación de cada técnica.\nLogra interpretar desde un punto de vista estadístico y sociológico los resultados derivados de pruebas estadísticas para analizar la relación entre dos variables.\nEs capaz de reportar y comunicar adecuada y eficientemente los resultados de los análisis estadísticos"
  },
  {
    "objectID": "syllabus.html#saberes-contenidos",
    "href": "syllabus.html#saberes-contenidos",
    "title": "Programa",
    "section": "Saberes / Contenidos",
    "text": "Saberes / Contenidos\n\nUnidad I: Inferencia\n\nDatos, variables y probabilidad\nCurva normal y error estándar\nIntervalos de confianza\nTest de hipótesis\nHipótesis no direccionales y para proporciones\n\n\n\nUnidad II: Asociación\n\nAsociación y covarianza\nCorrelación de Pearson\nCorrelación con variables ordinales\nMatrices y tamaños de efecto en correlación\nAsociación con variables categóricas\n\n\n\nUnidad III: Reporte\n\nResponder problemas de investigación de lógica bivariada con datos reales\nEscritura de reportes de investigación\nVisualización de datos\nPresentación de resultados"
  },
  {
    "objectID": "syllabus.html#metodología",
    "href": "syllabus.html#metodología",
    "title": "Programa",
    "section": "Metodología",
    "text": "Metodología\n\nSesiones de clases lectivas presenciales semanales, donde se presentarán los aspectos centrales de los contenidos correspondientes a la semana.\nPrácticos: los temas del curso se acompañan de guías prácticas de aplicación de contenidos. Estas guías están diseñadas para ser desarrolladas de manera autónoma, y también habrá espacio de revisión y consultas en el espacio de clases.\nTrabajos: se desarrollarán trabajos de investigación que permitirán a l_s participantes aplicar contenidos y recibir retroalimentación de su desempeño. Los trabajos serán asesorados por ayudantes que se asignarán a cada grupo.\n\nEl semestre comienza con clases lectivas, y posteriormente se integran elementos prácticos y de aplicación.\n\nLas clases en general se acompañan de documentos de presentación, que estarán disponibles antes de la sesión en la página de Clases, y están desarrollados con base en Rmarkdown/Xaringan. Estos documentos no son:\n\n“la clase”\nautoexplicativos (ni aspiran a serlo)\n“el ppt” (ni menos “la ppt”)"
  },
  {
    "objectID": "syllabus.html#evaluación",
    "href": "syllabus.html#evaluación",
    "title": "Programa",
    "section": "Evaluación",
    "text": "Evaluación\nEl curso tendrá tres instancias de evaluación:\n\nEvaluación 1: Inferencia (35% = 30% teórico + 5% práctico).\nEvaluación 2: Asociación (35% = 30% teórico + 5% práctico)\nEvaluación 3: Reporte de aplicación - trabajo grupal (30%= 20% reporte escrito + 10% poster)\n\nLa nota ponderada de las evaluaciones equivaldrá al 60% de la nota del curso y el examen final al 40% restante.\nLas evaluaciones se distribuyen en el semestre de la siguiente manera:\n\n\n\n\n\n\n\nATENCIÓN\n\n\n\nLas fechas de evaluación no se cambian por respeto a la planificación de los tiempos de tod_s quienes participan en el curso y el cumplimiento apropiado de los objetivos de aprendizaje."
  },
  {
    "objectID": "syllabus.html#inasistencias-y-atraso-en-entregas",
    "href": "syllabus.html#inasistencias-y-atraso-en-entregas",
    "title": "Programa",
    "section": "Inasistencias y atraso en entregas",
    "text": "Inasistencias y atraso en entregas\nLos justificativos por ausencia o atraso se realizan en la secretaría de carrera. Lo que la carrera informe como justificado, es lo que se va a considerar en el curso. No enviar justificativos a equipo docente y a ayudantes directamente, no es necesario ni apropiado para l_s estudiantes tener que exponer situaciones personales.\nEn caso de faltar a alguna de las evaluaciones existirá una única fecha para evaluaciones recuperativas. Si en esa fecha no es posible asistir por motivos justificados, entonces pasará directo a examen.\nEn el caso de los trabajos, en caso de atraso se descontará 0.5 por día adicional. Si el trabajo no se entrega luego del tercer día de atraso será calificado con nota 1.0"
  },
  {
    "objectID": "syllabus.html#requisitos-de-aprobación",
    "href": "syllabus.html#requisitos-de-aprobación",
    "title": "Programa",
    "section": "Requisitos de aprobación",
    "text": "Requisitos de aprobación\nRequisitos de eximición a examen:\n\ncontar con un promedio ponderado igual o superior a 5.5\nno tener nota bajo 4.0 en ninguna de las evaluaciones\n\nRequisitos para presentación a examen:\n\nPodrán presentarse al examen de primera oportunidad los estudiantes que hayan obtenido una calificación final igual o superior a 3.5.\nEl examen de segunda oportunidad será para aquellos estudiantes que presenten una nota igual o inferior a 3.5 o aquellos que en el examen de primera oportunidad no hubiesen logrado una nota igual o superior a 4.0."
  },
  {
    "objectID": "syllabus.html#bibliografía-obligatoria",
    "href": "syllabus.html#bibliografía-obligatoria",
    "title": "Programa",
    "section": "Bibliografía Obligatoria",
    "text": "Bibliografía Obligatoria\nCapítulos correspondientes a cada sesión de los siguientes textos principales:\n\nRitchey, F. (2008) Estadística para las ciencias sociales. McGraw-Hill: México.\nMoore (2010) Estadística aplicada básica. Barcelona: Antoni Bosch.\nPardo, Ruiz y San Martín (2015). Análisis de Datos en Ciencias Sociales y de la Salud I. Editorial Síntesis: Madrid."
  },
  {
    "objectID": "syllabus.html#bibliografía-complementaria",
    "href": "syllabus.html#bibliografía-complementaria",
    "title": "Programa",
    "section": "Bibliografía Complementaria",
    "text": "Bibliografía Complementaria\nWickham, H., & Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data (First edition). Sebastopol: O’Reilly.\nField, A., Milles, J., & Field, Z. (2012). Discovering statistics using R. London: Sage.\nSalkind, N. J. (Ed.). (2010). Encyclopedia of research design (Vol. 1). Sage.\nLevin, J. & Levin, W. (1997). Fundamentos de Estadística en la Investigación Social (Vol.2). Oxford University Press."
  },
  {
    "objectID": "syllabus.html#sobre-participación-y-comunicación",
    "href": "syllabus.html#sobre-participación-y-comunicación",
    "title": "Programa",
    "section": "Sobre participación y comunicación",
    "text": "Sobre participación y comunicación\n\nSe espera asistencia y participación activa, tanto a las sesiones lectivas como a las prácticas. Se pasará lista en todas las sesiones. No habrá penalización por inasistencia, pero si llevaremos registro principalmente con objetivos de monitoreo y retroalimentación del curso.\nInformar flexibilidades académicas al principio del semestre (en caso que la jefatura de carrera no lo haga de manera centralizada). Las flexibilidades academicas no aplican para cambios de fechas de evaluaciones grupales.\nSe espera y enfatiza la participación activa por distintos canales disponibles. Estos son:\n\ncontacto por correo con equipo docente del curso (profesor y apoyos docentes)\nespacio para resolver dudas individualmente al final de la clase\nreuniones con equipo docente, para lo cual se deben inscribir previamente en la página inicial de este sitio\nforos, disponibles tanto para las clases como para los prácticos.\n\nTambién se enfatiza un trato respetuoso y horizontal. Quienes están tomando este curso serán referidos como participantes y/o estudiantes, evitar el uso de “l_s cabr_s” o “l_s chiquill_s”, que si bien puede intentar transmitir cercanía finalmente expresan minimización de la contraparte. Quien no se sienta tratad_ apropiadamente o vea que otr_s no lo están siendo, se solicita reportar para solucionar la situación."
  },
  {
    "objectID": "syllabus.html#programación-de-sesiones",
    "href": "syllabus.html#programación-de-sesiones",
    "title": "Programa",
    "section": "Programación de sesiones",
    "text": "Programación de sesiones\nVisitar la página de Planificación."
  },
  {
    "objectID": "news/2024-08-19_inicio.html",
    "href": "news/2024-08-19_inicio.html",
    "title": "Informaciones por acá",
    "section": "",
    "text": "← News\n\n\n\nEstimad_s estudiantes, acá en esta pestaña quedará registro de las informaciones y actualizaciones del curso. De todas maneras se enviará link por correo a UCursos cuando haya noticias relevantes."
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Code examples",
    "section": "",
    "text": "Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.\nMany sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!"
  },
  {
    "objectID": "content/foro-clases.html",
    "href": "content/foro-clases.html",
    "title": "Foro clases",
    "section": "",
    "text": "Foro para compartir dudas, aclaraciones, sugerencias sobre los contenidos del curso vistos en clases. Para poder participar en el foro hay que abir una cuenta en Github",
    "crumbs": [
      "Clases",
      "Foro Clases"
    ]
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Asociación con variables categóricas",
    "section": "",
    "text": "Documento de presentación"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Inferencia 3 - Hipótesis y prueba t",
    "section": "",
    "text": "Documento de presentación"
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Inferencia 4 - Test de hipótesis",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nLink al Foro de Clases",
    "crumbs": [
      "Clases",
      "Presentaciones",
      "Inferencia 4"
    ]
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Inferencia 2: Curva normal y error estándar",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nLink al Foro de Clases",
    "crumbs": [
      "Clases",
      "Presentaciones",
      "Inferencia 2"
    ]
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Presentación",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nForo\nEn caso de preguntas sobre las clases hacerlas en Foro Clases",
    "crumbs": [
      "Clases",
      "Presentaciones",
      "Introducción"
    ]
  },
  {
    "objectID": "assignment/foro-practicos.html",
    "href": "assignment/foro-practicos.html",
    "title": "Foro prácticos",
    "section": "",
    "text": "Foro para compartir dudas, aclaraciones, sugerencias sobre llas guías prácticas. Para poder participar en el foro hay que abir una cuenta en Github\n\n¿Cuándo usar este foro?\n\n\n\n\n\nMuchas veces sucede que los códigos de análisis no resultan, lo que puede deberse a errores menores en la escritura de código, otras a versiones de librerías, y la mayoría a que R es algo mañoso.\nCuando hay un error o el código no corre lo importante es evitar que la frustración lleve a desmotivarse o tirar el computador por la ventana. Por eso se sugiere:\n\nintentar resolverlo por no más de 10 minutos: en este tiempo revisar bien el material disponible y lo que hay en la web. Un lugar clásico donde se discuten problemas de código es Stack overflow.\nsi no se logra solucionar entonces se sugiere encarecidamente usar el foro al final de esta página, ya que nos sirve para dejar la respuesta disponible para otr_s compañer_s que pueden tener la misma duda (generalmente es así)\n\n\n\n¿Cómo preguntar?\n\nDesripción general del problema, código y error que ocurre\nSi con esta información no basta para solucionar el problema, entonces se le podrá solicitar información adicionales, tales como el archivo de código y la información de la versión de las librerías que aparece al ejecutar el comando sessionInfo()\n\n\n\nForo prácticos",
    "crumbs": [
      "Prácticos",
      "Foro Prácticos"
    ]
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Prácticos",
    "section": "",
    "text": "Las clases se acompañan de guías de trabajo con énfasis en la aplicación práctica mediante el uso de software estadístico.\nLas guías se encontrarán disponibles en esta página (link a la izquierda). Habrá dos guías para la Unidad 1 (Inferencia), y 2 para la Unidad 2 (Asociación). La unidad 3 será eminentemente práctica y de aplicación.\nLas guías son desarrolladas de manera autónoma, y cada dos semanas los días martes habrá un espacio práctico de revisión de las guías y de consultas. Para ello se espera que quienes puedan traigan su computador a la sala, y quienes no tienen lo podrán hacer simultáneamente en la sala de computación 345.\nEn las prácticas vamos a trabajar con el software R, Versión 4.4.1.",
    "crumbs": [
      "Prácticos",
      "Descripción"
    ]
  },
  {
    "objectID": "assignment/index.html#descripción",
    "href": "assignment/index.html#descripción",
    "title": "Prácticos",
    "section": "",
    "text": "Las clases se acompañan de guías de trabajo con énfasis en la aplicación práctica mediante el uso de software estadístico.\nLas guías se encontrarán disponibles en esta página (link a la izquierda). Habrá dos guías para la Unidad 1 (Inferencia), y 2 para la Unidad 2 (Asociación). La unidad 3 será eminentemente práctica y de aplicación.\nLas guías son desarrolladas de manera autónoma, y cada dos semanas los días martes habrá un espacio práctico de revisión de las guías y de consultas. Para ello se espera que quienes puedan traigan su computador a la sala, y quienes no tienen lo podrán hacer simultáneamente en la sala de computación 345.\nEn las prácticas vamos a trabajar con el software R, Versión 4.4.1.",
    "crumbs": [
      "Prácticos",
      "Descripción"
    ]
  },
  {
    "objectID": "assignment/index.html#trabajo-con-software-r",
    "href": "assignment/index.html#trabajo-con-software-r",
    "title": "Prácticos",
    "section": "Trabajo con software R",
    "text": "Trabajo con software R\nPara los análisis estadísticos de este curso usamos el programa R, en parte porque es gratuito, pero la principal razón es que es de código abierto. Esto quiere decir que cualquier persona puede revisar cómo está hecho y aportar con modificaciones y procedimientos nuevos, como son las librerías que realizan funciones específicas.\nEl carácter de apertura de R posee muchas ventajas, pero también conlleva complicaciones. Se actualiza permanentemente, así como también las librerías, y esto puede generar problemas de compatibilidad y de fallas en ejecución del código de análisis.\nPara minimizar estos posibles problemas en este curso, vamos a:\n\ntrabajar con la misma y última versión de R\nevitar uso de tilde, ñ, espacios y mayúsculas tanto en carpetas y archivos, así como también en los nombres de las variables",
    "crumbs": [
      "Prácticos",
      "Descripción"
    ]
  },
  {
    "objectID": "assignment/index.html#sobre-errores-y-consultas-sobre-problemas-con-r-y-ejecución-de-código",
    "href": "assignment/index.html#sobre-errores-y-consultas-sobre-problemas-con-r-y-ejecución-de-código",
    "title": "Prácticos",
    "section": "Sobre errores y consultas sobre problemas con R y ejecución de código",
    "text": "Sobre errores y consultas sobre problemas con R y ejecución de código\nEn caso de preguntas sobre las clases hacerlas en Foro prácticos\n\nInstalación de R & RStudio\nPara esta versión del curso vamos a trabajar con el programa R Version 4.4.1 y con RStudio, que ofrece un entorno más amigable para trabajar con R.\nPara instalar R: ir a https://cran.r-project.org/index.html y bajar/instalar la versión correspondiente a la plataforma utilizada (Windows, Mac o Linux)\nPara instalar RStudio: ir a https://rstudio.com/products/rstudio/ y bajar/instalar RStudio desktop, Open Source License (libre).\nEn caso de dudas se puede revisar el siguiente video tutorial de instalación de R & RStudio, preparado por Julio Iturra (apoyo docente) del curso Estadística Multivariada 2020:\n\n\n\n\n\nSi por alguna razón se prefiere trabajar sin descargar, también se puede utilizar RCloud, abajo un tutorial preparado por Valentina Andrade para el curso de Estadística Multivariada:\n\n\n\n\n\n\nSobre el trabajo en hojas de código en RStudio\n\nEl trabajo de análisis en RStudio se efectua en una hoja de código (o R script o sintaxis, o para los usuarios de Stata la do-file), que es donde se anotan los comandos y funciones. Para abrir una hoja, en RStudio ir a File &gt; New File &gt; R Script (o ctrl+shift+N),y aparecerá un panel con una pestaña “Untitled” (sin título). Esta es la hoja de código donde se anotan los comandos.\nLos contenidos de las hojas de código son básicamente 2:\n\ncomandos o funciones: se escriben en la hoja, y para ejecutarlos se debe posicionar el cursor en la línea respectiva y ctrl+enter, el resultado aparecerá en el panel de resultados o Consola.\ntexto: para escribir títulos, comentarios, y todo lo que permita entender qué se está haciendo, al principio de la línea respectiva escribir el signo #\n\nPara grabar nuestra hoja de código y así respaldar nuestros análisis, File &gt; Save (o ctrl+s), y dar un nombre al archivo. Recordar: breve, sin espacios ni tildes ni eñes. Por defecto, la extensión de estos archivos es .R",
    "crumbs": [
      "Prácticos",
      "Descripción"
    ]
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Inferencia 1: Datos, probabilidad y distribuciones muestrales",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nLink al Foro de Clases"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Inferencia 3 - Intervalos de confianza",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nLink al Foro de Clases",
    "crumbs": [
      "Clases",
      "Presentaciones",
      "Inferencia 3"
    ]
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Inferencia 2 - Error e intervalos de confianza",
    "section": "",
    "text": "Documento de presentación"
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Inferencia 4 - Hipótesis direccionales y proporciones",
    "section": "",
    "text": "Documento de presentación"
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Cierre",
    "section": "",
    "text": "Documento de presentación"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Clases",
    "section": "",
    "text": "En esta sección se encuentran disponibles los documentos de presentación que sirven de base a cada clase, en el menú de la izquierda Presentaciones. Los documentos son en formato html (no son ppt), producidos con Xaringan. Para verlos en pantalla completa presionar F sobre el documento, y para una vista general de todas las slides presionar O.\nTambién a la izquierda hay un link al Foro para hacer preguntas relacionadas con las clases.\nCada clase tiene como referencia lecturas que deben completarse antes de la sesión correspondiente.",
    "crumbs": [
      "Clases",
      "Descripción"
    ]
  },
  {
    "objectID": "news/2024-08-19_cambio-practico.html",
    "href": "news/2024-08-19_cambio-practico.html",
    "title": "Prácticos parten próxima semana (Martes 27)",
    "section": "",
    "text": "← News\n\n\n\nInicialmente en el programa estaba contemplado que esta semana el martes 20 comenzaba la primera sesión de prácticos, para lo cual debían traer sus computadores. Esto se modifica, el práctico es el próximo martes 27."
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Recursos",
    "section": "",
    "text": "En esta sección se irán subiendo una serie de recursos relacionados con el curso."
  },
  {
    "objectID": "resource/index.html#glosario-de-conceptos",
    "href": "resource/index.html#glosario-de-conceptos",
    "title": "Recursos",
    "section": "Glosario de conceptos",
    "text": "Glosario de conceptos\n\n\n\nConcepto\nDefinición\n\n\n\n\nEstadística \nConjunto de métodos y herramientas que involucra la recopilación, análisis, interpretación y presentación de datos numéricos con el objetivo de describir patrones, relaciones y tendencias en fenómenos naturales o sociales.\n\n\nReproducibilidad \nLa capacidad de regenerar un experimento, análisis o estudio utilizando los mismos datos y métodos para llegar a los mismos resultados originales, verificando y asegurando la validez de los hallazgos.\n\n\nCiencia Social Abierta \nUn enfoque en la investigación social que promueve la transparencia, el acceso abierto a datos, métodos y resultados, y la colaboración entre investigadores para mejorar la calidad y confiabilidad de la investigación.\n\n\nProtocolo IPO (Input-Process-Output) \nSistema digital de carpetas interconectadas: entrada, proceso y salida. Se utiliza para organizar, procesar y documentar los datos y código de un proyecto de investigación para que cualquier persona pueda ejecutarlo y compartirlo.\n\n\nR project \nCarpeta raíz organizada donde trabajas en un proyecto concreto en el lenguaje de programación R, permitiéndote gestionar archivos, paquetes y configuraciones de manera específica para ese proyecto.\n\n\nTexto plano \nTipo de texto sin formato especial que se puede leer independiente del lector que se utilice.\n\n\nMarkdown \nClase especial de lenguaje que permite darle formato a texto simple con pocas marcas. Se utiliza comúnmente para escribir documentos simples con formato, como páginas web, documentación y presentaciones.\n\n\nDocumentos dinámicos \nArchivos que combinan texto plano y código de análisis (gráficos, tablas y resultados), de manera simultánea en un solo documento, permitiendo la generación automática y reproducible de resultados actualizados a medida que cambian los datos o parámetros.\n\n\nRMarkdown \nUna extensión de Markdown en el entorno R que permite la integración simultánea de texto plano y código R y su ejecución en el documento, lo que facilita la creación de documentos dinámicos con análisis estadísticos y visualizaciones.\n\n\nLibrerías \nConjuntos de funciones y herramientas predefinidas que se pueden utilizar en lenguajes de programación, como R, para realizar tareas específicas sin tener que escribir todo el código desde cero.\n\n\nKnitear \nProceso de compilación secuencial de código y resultados de ejecución en un documento RMarkdown, generando un documento final con texto formateado, código y gráficos integrados.\n\n\nRenderizar \nEn el contexto de RMarkdown se refiere al proceso de convertir el código y contenido en un documento legible y presentable. En otras palabras, cuando renderizas un documento RMarkdown, estás transformando el código, texto y elementos visuales en un formato final, como un informe, una presentación o un documento HTML, que pueda ser compartido o presentado a otros de manera comprensible.\n\n\nYAML \nAcrónimo de “YAML Ain’t Markup Language”, es un formato de serialización de datos legible por humanos que se utiliza para configurar y definir la estructura de datos en muchos programas y aplicaciones. En RMarkdown corresponden al encabezado de instrucciones generales del documento.\n\n\nChunk \nUn bloque de código, que puede ser en R, en un documento RMarkdown, rodeado por marcas especiales que indican al sistema cómo manejar y ejecutar ese fragmento de código, y luego mostrar sus resultados en el documento final."
  },
  {
    "objectID": "resource/index.html#reporte",
    "href": "resource/index.html#reporte",
    "title": "Recursos",
    "section": "Reporte",
    "text": "Reporte\n\nTablas con R, con ejemplo de canciones de Spotify\nIntroduction to Quarto"
  },
  {
    "objectID": "resource/index.html#estadística-descriptiva",
    "href": "resource/index.html#estadística-descriptiva",
    "title": "Recursos",
    "section": "Estadística descriptiva",
    "text": "Estadística descriptiva\n\nCurso Estadística Descriptiva Sociología UChile, 1er Sem 2023"
  },
  {
    "objectID": "resource/index.html#uso-de-r",
    "href": "resource/index.html#uso-de-r",
    "title": "Recursos",
    "section": "Uso de R",
    "text": "Uso de R\n\nConocimientos básicos de programación en R\nImportar datos en R\nProcesamiento y análisis de datos en R (tidyverse)\nProcesamiento y análisis de datos en R (base)\nMás para aprender R"
  },
  {
    "objectID": "resource/index.html#inferencia",
    "href": "resource/index.html#inferencia",
    "title": "Recursos",
    "section": "Inferencia",
    "text": "Inferencia\n\nPor qué se divide la varianza por N-1?\nStatistical Inference via Data Science A ModernDive into R and the Tidyverse\nIntroduction to modern statistics (Mine Çetinkaya-Rundel and Johanna Hardin)\nInferencia univariada"
  },
  {
    "objectID": "resource/index.html#visualización",
    "href": "resource/index.html#visualización",
    "title": "Recursos",
    "section": "Visualización",
    "text": "Visualización\n\nVisualización descriptiva de datos en R\nR Graph Gallery"
  },
  {
    "objectID": "resource/index.html#bases-de-datos",
    "href": "resource/index.html#bases-de-datos",
    "title": "Recursos",
    "section": "Bases de datos",
    "text": "Bases de datos\n\nBases de datos para trabajos o investigación"
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "Informaciones",
    "section": "",
    "text": "Acá las principales informaciones y actualizaciones del curso.\n\n\n\n   \n     \n     \n       Ordenar por\n       Por defecto\n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - Más reciente\n        \n         \n          Título\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nFecha\n\n\nTítulo\n\n\nCategorías\n\n\n\n\n\n\nviernes agosto 23, 2024 at 12:00 AM\n\n\nSesión práctica martes 27 de agosto\n\n\ninfo,mail-UCursos\n\n\n\n\nlunes agosto 19, 2024 at 12:00 AM\n\n\nPrácticos parten próxima semana (Martes 27)\n\n\ninfo\n\n\n\n\nlunes agosto 19, 2024 at 12:00 AM\n\n\nInformaciones por acá\n\n\ninfo\n\n\n\n\n\n\nNo hay resultados\n\n\n\n\n\n\n\n\nSuscribirse!\n\n\n\nPuedes usar un lector de feeds como Feedly o un servicio RSS-to-email como Blogtrottr para suscribirte a cualquiera de estos mensajes.\n\n\n\n\n\n RSS"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Planificación",
    "section": "",
    "text": "Este curso se compone de tres actividades principales:\n\n Clases lectivas presenciales: donde en base a las lecturas correspondientes a esa semana se presentará un resumen de los contenidos principales y se resolverán dudas.\n Guías prácticas: actividades con énfasis en el manejo de software para análisis y reporte de los resultados. Estas instancias serán conducidas guiadas por los apoyos docentes del curso.\n Lecturas: los temas del curso se acompañan de lecturas, las que se encuentras a disposición en esta página.\n\nLas actividades semanales se resumen en el siguiente esquema:\n\nEn cuanto a la metodología, el curso comienza con clases lectivas y desarrollo autónomo de guías prácticas, y en la última unidad se realizará un trabajo grupal práctico de aplicación de los contenidos.\n\n\n\n\n\n\n\n\n\n\n Agosto \n Clases\n Prácticos\n Lecturas\n\n\nLunes 12\n1. Presentación\n\nLeer detalladamente programa del curso\n\n\n\n\n\n\n\n\n\n\n Agosto \n Clases\n Prácticos\n Lecturas\n\n\nMartes 13\nDatos, probabilidad y distribuciones muestrales\n\n*Pardo cap 2 Conceptos previos\nRichtey 1-21 : La imaginación estadística\n\n\nLunes 19\nError estándar y distribución normal\n\n*Richtey cap 6\nPardo cap 6 Probabilidad y distribución normal\n\n\nMartes 20\nIntervalos de confianza\n\n*Richtey cap 7 Probabilidad y distribuciones muestrales\n\n\nLunes 26\nTest de hipótesis\n\n*Richtey cap 8: Intervalos de confianza\n*Richtey cap 9: Prueba de hipótesis\nMontoya: Los conceptos de especificación y falsación\n\n\nMartes 27\n\nPráctico: Inferencia 1\n\n\n\n Septiembre \n\n\n\n\n\nLunes 2\nHipótesis para una y dos muestras\n\n*Richtey cap 10: Hipótesis de una muestra\n*Richtey cap 11: Hipótesis de dos muestras (prueba t)\n\n\nMartes 3\n\nPráctico Inferencia 2\n\n\n\nLunes 9\nEvaluación 1\n\n\n\n\nMartes 10\nEvaluación 1\n\n\n\n\n\n\n\n\n\n\nLunes 23\nAsociación y covarianza\n\n\n\n\nMartes 24\nInferencia en correlación de Pearson\n\n\n\n\nLunes 30\nCorrelación con ordinales\n\n\n\n\nMartes 1\n\nPráctico. Guía 1 Bivariada\n\n\n\nLunes 7\nMatrices y tamaños de efecto en correlación\n\n\n\n\nMartes 8\nAsociación con categóricas 1\n\n\n\n\nLunes 14\nAsociación con categóricas 2\n\n\n\n\nMartes 15\n\nPráctico: Guía 2 Bivariada\n\n\n\nLunes 21\nEvaluación 2\n\n\n\n\nMartes 22\nEvaluación 2\n\n\n\n\nLunes 28\nSemana receso\n\n\n\n\nMartes 29\nSemana receso\n\n\n\n\n\n\n\n\n\n\nLunes 4\nSeminario encuestas\n\n\n\n\nMartes 5\n\nDefinición de grupos y temas\n\n\n\nLunes 11\nEscritura de reportes de investigación\n\n\n\n\nMartes 12\n\nReportes dinámicos y\nPlantilla trabajo final\n\n\n\nLunes 18\nVisualización de datos 1\n\n\n\n\nMartes 19\n\nVisualización 1\n\n\n\nLunes 25\nVisualización de datos 2\n\n\n\n\nMartes 26\n\nVisualización 2 y elaboración de poster\n\n\n\nLunes 25\nPresentación de poster de investigación\n\n\n\n\nMartes 26\nPruebas recuperativas\n\n\n\n\nViernes 29\nEntrega de trabajo final grupal"
  },
  {
    "objectID": "schedule.html#forma-general-de-funcionamiento",
    "href": "schedule.html#forma-general-de-funcionamiento",
    "title": "Planificación",
    "section": "",
    "text": "Este curso se compone de tres actividades principales:\n\n Clases lectivas presenciales: donde en base a las lecturas correspondientes a esa semana se presentará un resumen de los contenidos principales y se resolverán dudas.\n Guías prácticas: actividades con énfasis en el manejo de software para análisis y reporte de los resultados. Estas instancias serán conducidas guiadas por los apoyos docentes del curso.\n Lecturas: los temas del curso se acompañan de lecturas, las que se encuentras a disposición en esta página.\n\nLas actividades semanales se resumen en el siguiente esquema:\n\nEn cuanto a la metodología, el curso comienza con clases lectivas y desarrollo autónomo de guías prácticas, y en la última unidad se realizará un trabajo grupal práctico de aplicación de los contenidos.\n\n\n\n\n\n\n\n\n\n\n Agosto \n Clases\n Prácticos\n Lecturas\n\n\nLunes 12\n1. Presentación\n\nLeer detalladamente programa del curso\n\n\n\n\n\n\n\n\n\n\n Agosto \n Clases\n Prácticos\n Lecturas\n\n\nMartes 13\nDatos, probabilidad y distribuciones muestrales\n\n*Pardo cap 2 Conceptos previos\nRichtey 1-21 : La imaginación estadística\n\n\nLunes 19\nError estándar y distribución normal\n\n*Richtey cap 6\nPardo cap 6 Probabilidad y distribución normal\n\n\nMartes 20\nIntervalos de confianza\n\n*Richtey cap 7 Probabilidad y distribuciones muestrales\n\n\nLunes 26\nTest de hipótesis\n\n*Richtey cap 8: Intervalos de confianza\n*Richtey cap 9: Prueba de hipótesis\nMontoya: Los conceptos de especificación y falsación\n\n\nMartes 27\n\nPráctico: Inferencia 1\n\n\n\n Septiembre \n\n\n\n\n\nLunes 2\nHipótesis para una y dos muestras\n\n*Richtey cap 10: Hipótesis de una muestra\n*Richtey cap 11: Hipótesis de dos muestras (prueba t)\n\n\nMartes 3\n\nPráctico Inferencia 2\n\n\n\nLunes 9\nEvaluación 1\n\n\n\n\nMartes 10\nEvaluación 1\n\n\n\n\n\n\n\n\n\n\nLunes 23\nAsociación y covarianza\n\n\n\n\nMartes 24\nInferencia en correlación de Pearson\n\n\n\n\nLunes 30\nCorrelación con ordinales\n\n\n\n\nMartes 1\n\nPráctico. Guía 1 Bivariada\n\n\n\nLunes 7\nMatrices y tamaños de efecto en correlación\n\n\n\n\nMartes 8\nAsociación con categóricas 1\n\n\n\n\nLunes 14\nAsociación con categóricas 2\n\n\n\n\nMartes 15\n\nPráctico: Guía 2 Bivariada\n\n\n\nLunes 21\nEvaluación 2\n\n\n\n\nMartes 22\nEvaluación 2\n\n\n\n\nLunes 28\nSemana receso\n\n\n\n\nMartes 29\nSemana receso\n\n\n\n\n\n\n\n\n\n\nLunes 4\nSeminario encuestas\n\n\n\n\nMartes 5\n\nDefinición de grupos y temas\n\n\n\nLunes 11\nEscritura de reportes de investigación\n\n\n\n\nMartes 12\n\nReportes dinámicos y\nPlantilla trabajo final\n\n\n\nLunes 18\nVisualización de datos 1\n\n\n\n\nMartes 19\n\nVisualización 1\n\n\n\nLunes 25\nVisualización de datos 2\n\n\n\n\nMartes 26\n\nVisualización 2 y elaboración de poster\n\n\n\nLunes 25\nPresentación de poster de investigación\n\n\n\n\nMartes 26\nPruebas recuperativas\n\n\n\n\nViernes 29\nEntrega de trabajo final grupal"
  },
  {
    "objectID": "schedule.html#exámenes-finales-escritos-en-sala",
    "href": "schedule.html#exámenes-finales-escritos-en-sala",
    "title": "Planificación",
    "section": "Exámenes finales (escritos, en sala)",
    "text": "Exámenes finales (escritos, en sala)\nVer requisitos de aprobación y eximición\n\nExamen de primera oportunidad: Lunes 9 Diciembre 10:15\nExamen de segunda oportunidad: Lunes 16 Diciembre 10:15"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Estadística Correlacional\n        ",
    "section": "",
    "text": "Inferencia, asociación y reporte\n        \n        \n            SOC01019 • Segundo Semestre 2024Departamento de Sociología, Facultad de Ciencias SocialesUniversidad de Chile"
  },
  {
    "objectID": "example/cace.html",
    "href": "example/cace.html",
    "title": "Complier average treatment effects",
    "section": "",
    "text": "Throughout this course, we’ve talked about the difference between the average treatment effect (ATE), or the average effect of a program for an entire population, and conditional average treatment effect (CATE), or the average effect of a program for some segment of the population. There are all sorts of CATEs: you can find the CATE for men vs. women, for people who are treated with the program (the average treatment on the treated, or ATT or TOT), for people who are not treated with the program (the average treatment on the untreated, or ATU), and so on.\nOne important type of CATE is the effect of a program on just those who comply with the program. We can call this the complier average treatment effect, but the acronym would be the same as conditional average treatment effect, so we’ll call it the complier average causal effect or CACE.\nThinking about compliance is important. You might randomly assign people to receive treatment or a program, but people might not do what you tell them. Additionally, people might do the program if assigned to do it, but they would have done it anyway. We can split the population into four types of people:\n\nCompliers: People who follow whatever their assignment is (if assigned to treatment, they do the program; if assigned to control, they don’t)\nAlways takers: People who will receive or seek out the program regardless of assignment (if assigned to treatment, they do the program; if assigned to control, they still do the program)\nNever takers: People who will not receive or seek out the program regardless of assignment (if assigned to treatment, they don’t do the program; if assigned to control, they also don’t do it)\nDefiers: People who will do the opposite of whatever their assignment is (if assigned to treatment, they don’t do the program; if assigned to control, they do the program)\n\nTo simplify things, evaluators and econometricians assume that defiers don’t exist based on the idea of monotonicity, which means that we can assume that the effect of being assigned to treatment only increases the likelihood of participating in the program (and doesn’t make it more likely).\nThe tricky part about trying to find who the compliers are in a sample is that we can’t know what people would have done in the absence of treatment. If we see that someone in the experiment was assigned to be in the treatment group and they then participated in the program, they could be a complier (since they did what they were assigned to do), or they could be an always taker (they did what they were assigned to do, but they would have done it anyway). Due to the fundamental problem of causal inference, we cannot know what each person would have done in a parallel world.\nWe can use data from a hypothetical program to see how these three types of compliers distort our outcomes, and more importantly, how we can disentangle compliers from their always- and never-taker counterparts.\nIf you want to follow along with this example, you can download these two datasets:\n\n bed_nets_time_machine.csv\n bed_nets_observed.csv"
  },
  {
    "objectID": "example/cace.html#compliance-and-treatment-effects",
    "href": "example/cace.html#compliance-and-treatment-effects",
    "title": "Complier average treatment effects",
    "section": "",
    "text": "Throughout this course, we’ve talked about the difference between the average treatment effect (ATE), or the average effect of a program for an entire population, and conditional average treatment effect (CATE), or the average effect of a program for some segment of the population. There are all sorts of CATEs: you can find the CATE for men vs. women, for people who are treated with the program (the average treatment on the treated, or ATT or TOT), for people who are not treated with the program (the average treatment on the untreated, or ATU), and so on.\nOne important type of CATE is the effect of a program on just those who comply with the program. We can call this the complier average treatment effect, but the acronym would be the same as conditional average treatment effect, so we’ll call it the complier average causal effect or CACE.\nThinking about compliance is important. You might randomly assign people to receive treatment or a program, but people might not do what you tell them. Additionally, people might do the program if assigned to do it, but they would have done it anyway. We can split the population into four types of people:\n\nCompliers: People who follow whatever their assignment is (if assigned to treatment, they do the program; if assigned to control, they don’t)\nAlways takers: People who will receive or seek out the program regardless of assignment (if assigned to treatment, they do the program; if assigned to control, they still do the program)\nNever takers: People who will not receive or seek out the program regardless of assignment (if assigned to treatment, they don’t do the program; if assigned to control, they also don’t do it)\nDefiers: People who will do the opposite of whatever their assignment is (if assigned to treatment, they don’t do the program; if assigned to control, they do the program)\n\nTo simplify things, evaluators and econometricians assume that defiers don’t exist based on the idea of monotonicity, which means that we can assume that the effect of being assigned to treatment only increases the likelihood of participating in the program (and doesn’t make it more likely).\nThe tricky part about trying to find who the compliers are in a sample is that we can’t know what people would have done in the absence of treatment. If we see that someone in the experiment was assigned to be in the treatment group and they then participated in the program, they could be a complier (since they did what they were assigned to do), or they could be an always taker (they did what they were assigned to do, but they would have done it anyway). Due to the fundamental problem of causal inference, we cannot know what each person would have done in a parallel world.\nWe can use data from a hypothetical program to see how these three types of compliers distort our outcomes, and more importantly, how we can disentangle compliers from their always- and never-taker counterparts.\nIf you want to follow along with this example, you can download these two datasets:\n\n bed_nets_time_machine.csv\n bed_nets_observed.csv"
  },
  {
    "objectID": "example/cace.html#finding-compliers-with-a-mind-reading-time-machine",
    "href": "example/cace.html#finding-compliers-with-a-mind-reading-time-machine",
    "title": "Complier average treatment effects",
    "section": "Finding compliers with a mind-reading time machine",
    "text": "Finding compliers with a mind-reading time machine\nFirst let’s load the data and reorder some of the categories:\n\n\nCode\nlibrary(tidyverse)  # ggplot(), %&gt;%, mutate(), and friends\nlibrary(broom)  # Convert models to data frames\nlibrary(estimatr)  # Run 2SLS models in one step with iv_robust()\n\nbed_nets &lt;- read_csv(\"data/bed_nets_observed.csv\") %&gt;%\n  # Make \"No bed net\" (control) come first\n  mutate(bed_net = fct_relevel(bed_net, \"No bed net\"))\n\nbed_nets_time_machine &lt;- read_csv(\"data/bed_nets_time_machine.csv\") %&gt;%\n  # Make \"No bed net\" come first and \"Complier\" come first\n  mutate(bed_net = fct_relevel(bed_net, \"No bed net\"),\n         status = fct_relevel(status, \"Complier\"))\n\n\nThis is what we would be able to see if we could read everyone’s minds. There are always takers who will use a bed net regardless of the program, and they’ll have higher health outcomes. However, those better outcomes are because of something endogenous—there’s something else that makes these people always pursue bed nets, and that’s likely related to health. We probably want to not consider them when looking for the program effect. There are never takers who won’t ever use a bed net, and they have worse health outcomes. Again, there’s endogeneity here—something is causing them to not use the bed nets, and it likely also causes their health level. We don’t want to look at them either.\nThe first group—the compliers—are the people we want to focus on. Here we see that the program had an effect when compared to a control group.\n\n\nCode\nset.seed(1234)  # Make the jittering the same every time\n\nggplot(bed_nets_time_machine, aes(y = health, x = treatment)) +\n  geom_point(aes(shape = bed_net, color = status),\n             position = position_jitter(height = NULL, width = 0.25)) +\n  facet_wrap(vars(status)) +\n  labs(color = \"Type of person\", shape = \"Compliance\",\n       x = NULL, y = \"Health status\") +\n  scale_color_viridis_d(option = \"plasma\", end = 0.85) +\n  theme_bw()"
  },
  {
    "objectID": "example/cace.html#finding-compliers-in-actual-data",
    "href": "example/cace.html#finding-compliers-in-actual-data",
    "title": "Complier average treatment effects",
    "section": "Finding compliers in actual data",
    "text": "Finding compliers in actual data\nThis is what we actually see in the data, though. You can tell who some of the always takers are (those who used bed nets after being assigned to the control group) and who some of the never takers are (those who did not use a bed net after being assigned to the treatment group), but compliers are mixed up with the always and never takers. We have to somehow disentangle them!\n\n\nCode\nset.seed(1234)\nggplot(bed_nets_time_machine, aes(y = health, x = bed_net)) +\n  geom_point(aes(shape = bed_net, color = status),\n             position = position_jitter(height = NULL, width = 0.25)) +\n  facet_wrap(vars(treatment)) +\n  labs(color = \"Type of person\", shape = \"Compliance\",\n       x = NULL, y = \"Health status\") +\n  scale_color_viridis_d(option = \"plasma\", end = 0.85) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWe can do this by assuming the proportion of compliers, never takers, and always takers are equally spread across treatment and control (which we can assume through the magic of randomization). If that’s the case, we can calculate the intent to treat (ITT) effect, which is the CATE of being assigned treatment (or the effect of being assigned treatment on health status, regardless of actual compliance).\nThe ITT is actually composed of three different causal effects: the complier average causal effect (CACE), the always taker average causal effect (ATACE), and the never taker average causal effect (NTACE). In the formula below, \\(\\pi\\) stands for the proportion of people in each group. Formally, the ITT can be defined like this:\n\\[\n\\begin{aligned}\n\\text{ITT}\\ =\\ & \\color{#0D0887}{\\pi_\\text{compliers} \\times (\\text{T} - \\text{C})_\\text{compliers}} + \\\\\n&\\color{#B7318A}{\\pi_\\text{always takers} \\times (\\text{T} - \\text{C})_\\text{always takers}} + \\\\\n&\\color{#FEBA2C}{\\pi_\\text{never takers} \\times (\\text{T} - \\text{C})_\\text{never takers}}\n\\end{aligned}\n\\]\nWe can simplify this to this acronymized version:\n\\[\n\\text{ITT}\\ =\\ \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\text{ATACE}} + \\color{#FEBA2C}{\\pi_\\text{N} \\text{NTACE}}\n\\]\nThe number we care about the most here is the CACE, which is stuck in the middle of the equation. But we can rescue it with some fun logical and algebraic trickery!\nIf we assume that assignment to treatment doesn’t make someone more likely to be an always taker or a never taker, we can set the ATACE and NTACE to zero, leaving us with just three variables to worry about: ITT, \\(\\pi_\\text{c}\\), and CACE:\n\\[\n\\begin{aligned}\n\\text{ITT}\\ =\\ & \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\text{ATACE}} + \\color{#FEBA2C}{\\pi_\\text{N} \\text{NTACE}} \\\\[6pt]\n=\\ & \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\times 0} + \\color{#FEBA2C}{\\pi_\\text{N} \\times 0}\\\\[6pt]\n\\text{ITT}\\ =\\ & \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}}\n\\end{aligned}\n\\]\nWe can use algebra to rearrange this formula so that we’re left with an equation that starts with CACE (since that’s the value we care about):\n\\[\n\\text{CACE} = \\frac{\\text{ITT}}{\\pi_\\text{C}}\n\\]\nIf we can find the ITT and the proportion of compliers, we can find the complier average causal effect (CACE). Fortunately, both those pieces—ITT and \\(\\pi_\\text{C}\\)—are findable in the data we have!"
  },
  {
    "objectID": "example/cace.html#finding-the-itt",
    "href": "example/cace.html#finding-the-itt",
    "title": "Complier average treatment effects",
    "section": "Finding the ITT",
    "text": "Finding the ITT\nThe ITT is easy to find with a simple OLS model:\n\n\nCode\nitt_model &lt;- lm(health ~ treatment, data = bed_nets)\n\ntidy(itt_model)\n## # A tibble: 2 × 5\n##   term               estimate std.error statistic  p.value\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)           40.9      0.444     92.1  0       \n## 2 treatmentTreatment     5.99     0.630      9.51 5.20e-21\n\nITT &lt;- tidy(itt_model) %&gt;%\n  filter(term == \"treatmentTreatment\") %&gt;%\n  pull(estimate)\n\n\nThe ITT here is ≈6—being assigned treatment increases average health status by 5.99 health points."
  },
  {
    "objectID": "example/cace.html#finding-the-proportion-of-compliers",
    "href": "example/cace.html#finding-the-proportion-of-compliers",
    "title": "Complier average treatment effects",
    "section": "Finding the proportion of compliers",
    "text": "Finding the proportion of compliers\nThe proportion of compliers is a little trickier, but doable with some algebraic trickery. Recall from the graph above that the people who were in the treatment group and who complied are a combination of always takers and compliers. This means we can say:\n\\[\n\\begin{aligned}\n\\pi_\\text{A} + \\pi_\\text{C} =& \\text{% yes in treatment; or} \\\\\n\\pi_\\text{C} =& \\text{% yes in treatment} - \\pi_\\text{A}\n\\end{aligned}\n\\]\nWe actually know \\(\\pi_\\text{A}\\)—remember in the graph above that the people who were in the control group and who used bed nets are guaranteed to be always takers (none of them are compliers or never takers). If we assume that the proportion of always takers is the same in both treatment and control, we can use that percent here, giving us this final equation for \\(\\pi_\\text{C}\\):\n\\[\n\\begin{aligned}\n\\pi_\\text{C} =& \\text{% yes in treatment} - \\pi_\\text{A} \\\\\n=& \\text{% yes in treatment} - \\text{% yes in control}\n\\end{aligned}\n\\]\nSo, if we can find the percent of people assigned to treatment who used bed nets, find the percent of people assigned to control and used bed nets, and subtract the two percentages, we’ll have the proportion of compliers, or \\(\\pi_\\text{C}\\). We can do that with the data we have (61% - 19.5% = 41.5% compliers):\n\n\nCode\nbed_nets %&gt;%\n  group_by(treatment, bed_net) %&gt;%\n  summarize(n = n()) %&gt;%\n  mutate(prop = n / sum(n))\n## # A tibble: 4 × 4\n## # Groups:   treatment [2]\n##   treatment bed_net        n  prop\n##   &lt;chr&gt;     &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;\n## 1 Control   No bed net   808 0.805\n## 2 Control   Bed net      196 0.195\n## 3 Treatment No bed net   388 0.390\n## 4 Treatment Bed net      608 0.610\n\n# pi_c = prop yes in treatment - prop yes in control\npi_c &lt;- 0.6104418 - 0.1952191\n\n\nFinally, now that we know both the ITT and \\(\\pi_\\text{C}\\), we can find the CACE (or the LATE):\n\n\nCode\nCACE &lt;- ITT / pi_c\nCACE\n## [1] 14.43\n\n\nIt’s 14.4, which means that using bed nets increased health by 14 health points for compliers (which is a lot bigger than the 6 that we found before). We successfully filtered out the always takers and the never takers, and we have our complier-specific causal effect."
  },
  {
    "objectID": "example/cace.html#finding-the-cacelate-with-iv2sls",
    "href": "example/cace.html#finding-the-cacelate-with-iv2sls",
    "title": "Complier average treatment effects",
    "section": "Finding the CACE/LATE with IV/2SLS",
    "text": "Finding the CACE/LATE with IV/2SLS\nDoing that is super tedious though! What if there was an easier way to find the effect of the bed net program for just the compliers? We can do this with IV/2SLS regression by using assignment to treatment as an instrument.\nAssignment to treatment works as an instrument because it’s (1) relevant, since being told to use bed nets is probably highly correlated with using bed nets, (2) exclusive, since the only way that being told to use bed nets can cause changes in health is through the actual use of the bed nets, and (3) exogenous, since being told to use bed nets probably isn’t related to other things that cause health.\nHere’s a 2SLS regression with assignment to treatment as the instrument:\n\n\nCode\nmodel_2sls &lt;- iv_robust(health ~ bed_net | treatment, data = bed_nets)\ntidy(model_2sls)\n##             term estimate std.error statistic   p.value conf.low conf.high   df outcome\n## 1    (Intercept)    38.12    0.5151     74.01 0.000e+00    37.11     39.13 1998  health\n## 2 bed_netBed net    14.43    1.2538     11.51 1.038e-29    11.97     16.89 1998  health\n\n\nThe coefficient for bed_net is identical to the CACE that we found manually! Instrumental variables are helpful for isolated program effects to only compliers when you’re dealing with noncompliance."
  },
  {
    "objectID": "assignment/02-practico.html",
    "href": "assignment/02-practico.html",
    "title": "Cálculo y reporte de correlación",
    "section": "",
    "text": "El objetivo de esta guía práctica es aprender a calcular y graficar la correlación entre dos variables utilizando R.\nEn detalle, aprenderemos:\n\nQué es una correlación\nCuál es la correlación de Pearson\nCómo calcular una correlación de Pearson y graficarla"
  },
  {
    "objectID": "assignment/02-practico.html#diagrama-de-dispersión-nube-de-puntos-o-scatterplot",
    "href": "assignment/02-practico.html#diagrama-de-dispersión-nube-de-puntos-o-scatterplot",
    "title": "Cálculo y reporte de correlación",
    "section": "Diagrama de dispersión (nube de puntos o scatterplot)",
    "text": "Diagrama de dispersión (nube de puntos o scatterplot)\nSiempre es recomendable acompañar el valor de la correlación con una exploración gráfica de la distribución bivariada de los datos. El gráfico o diagrama de dispersión es una buena herramienta, ya que muestra la forma, la dirección y la fuerza de la relación entre dos variables cuantitativas.\nEste tipo de gráfico lo podemos realizar usando la librería ggplot2.\n\npacman::p_load(ggplot2)\nplot1 &lt;- ggplot(data, \n                aes(x=educ, y=ing)) + \n                geom_point(colour = \"red\", \n                size = 5)\nplot1\n\n\n\n\n\n\n\n\nEn el gráfico podemos ver como se crea una nube de puntos en las intersecciones de los valores para ambas variables de cada caso."
  },
  {
    "objectID": "assignment/02-practico.html#el-cuarteto-de-anscombe-y-las-limitaciones-de-la-correlación-lineal",
    "href": "assignment/02-practico.html#el-cuarteto-de-anscombe-y-las-limitaciones-de-la-correlación-lineal",
    "title": "Cálculo y reporte de correlación",
    "section": "El cuarteto de Anscombe y las limitaciones de la correlación lineal",
    "text": "El cuarteto de Anscombe y las limitaciones de la correlación lineal\nAhora, revisaremos un muy buen ejemplo de la importancia de la exploración gráfica de los datos mediante un ejemplo de Anscombe (1973), que permite visualizar las limitaciones del coeficiente de correlación.\nPrimero, crearemos la base de datos:\n\nanscombe &lt;- data.frame(\n  x1 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y1 = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),\n  x2 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y2 = c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),\n  x3 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73),\n  x4 = c(8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8),\n  y4 = c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)\n)\n\nCalculamos la correlación pares de datos\n\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\n\nPodemos observar que los valores de las correlaciones son equivalentes, por lo tanto podríamos pensar que todos los pares de columnas se encuentran correlacionados de manera similar.\nPero, ¿será suficiente con esa información? Pasemos a revisar los gráficos de dispersión de cada par de variables.\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n  geom_point(colour = \"red\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso I\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n  geom_point(colour = \"green\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso II\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n  geom_point(colour = \"yellow\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso III\")\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n  geom_point(colour = \"orange\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso IV\")\n\n\n\n\n\n\n\n\nComo vemos, con distintas distribuciones las correlaciones pueden ser las mismas, principalmente porque Pearson es una medida que solo captura relaciones lineales (rectas), además de verse influido fuertemente por valores extremos. Por lo mismo, es relevante siempre una buena visualización de la distribución bivariada de los datos como complemento al cálculo del coeficiente de correlación."
  },
  {
    "objectID": "assignment/09-practico.html",
    "href": "assignment/09-practico.html",
    "title": "Reportes dinámicos 3",
    "section": "",
    "text": "El objetivo de esta guía práctica es explicar el formato del informe de los trabajos del curso. Ya que la elaboración de los informes incluye analisis en R, la idea es poder aprovechar los recursos existentes para poder realizar escritura y análisis en un mismo documento. La base de esto son los prácticos sobre Reportes Dinámicos 1 y Reportes Dinámicos 2, donde se describieron las estructura de carpetas y archivos, así como también el lenfuage RMarkdown, que permite vincular análisis en R con escritura en Markdown.\nPara facilitar el desarrollo de los trabajos en este entorno ponemos a disposición una carpeta que se puede bajar aquí. Esta carpeta está basada en el protocolo IPO, mencionado en el práctico Reportes Dinámicos 1, con la siguente estructura de archivos y carpetas:\n\ninput: insumos, libro de códigos\ninput/img: imágenes\ninput/data: base de datos original\nprocesamiento: archivos de sintaxis .R\noutput: bases de datos procesada, informes, tablas, gráficos\n\n\nDonde el flujo de trabajo es el siguiente:\n\nEn detalle, aprenderemos los siguientes contenidos:\n\nCreación y organización de un informe en Quarto\nGeneración y presentación de tablas y gráficos en Quarto.\nCómo autoreferenciar elementos dentro un documento Quarto.\n\nPara los ejemplos de esta plantilla utilizaremos la encuesta ELSOC, pero como sabemos, el trabajo se realiza con la encuesta CASEN."
  },
  {
    "objectID": "assignment/09-practico.html#crear-un-documento-quarto",
    "href": "assignment/09-practico.html#crear-un-documento-quarto",
    "title": "Reportes dinámicos 3",
    "section": "Crear un documento Quarto",
    "text": "Crear un documento Quarto\nPara generar un documento Quarto hacemos lo siguiente:"
  },
  {
    "objectID": "assignment/09-practico.html#encabezado-o-yaml",
    "href": "assignment/09-practico.html#encabezado-o-yaml",
    "title": "Reportes dinámicos 3",
    "section": "Encabezado o YAML",
    "text": "Encabezado o YAML\nPara comenzar, definiremos los elementos que van en el encabezado (front matter o YAML), al menos debemos especificar:\n\nTítulo\nSubtítulo\nAutores\nFecha\nIdioma\n\n\nCon ese YAML el encabezado se ve de la siguiente forma:"
  },
  {
    "objectID": "assignment/09-practico.html#apartados-y-subapartados",
    "href": "assignment/09-practico.html#apartados-y-subapartados",
    "title": "Reportes dinámicos 3",
    "section": "Apartados y subapartados",
    "text": "Apartados y subapartados\nPara los títulos de los apartados deben poner un # antes del nombre, y para los subapartados cada vez más pequeños dos o más #.\nPor ejemplo, si ponemos:\n# Variables\n## Descripción de variables\nLos apartados y subapartados diferirán en su tamaño y se verán de esta forma."
  },
  {
    "objectID": "assignment/09-practico.html#insertar-imágenes",
    "href": "assignment/09-practico.html#insertar-imágenes",
    "title": "Reportes dinámicos 3",
    "section": "Insertar imágenes",
    "text": "Insertar imágenes\nSi quisiéramos poner una imagen, lo ideal es llamarla desde la web:\n\nRecordemos que si no queremos que se vea un chunk, en las options debemos poner echo=FALSE.\n\n\ninclude_graphics(\"https://www.desarrollosocialyfamilia.gob.cl/storage/image/banner-saludmental.png\")\n\n\n\n\n\n\n\n\nO desde la carpeta local ![](../input/img/nombre_imagen.extension) o con:\n\ninclude_graphics(\"../files/banner-saludmental.png\")"
  },
  {
    "objectID": "assignment/09-practico.html#carga-de-librerías",
    "href": "assignment/09-practico.html#carga-de-librerías",
    "title": "Reportes dinámicos 3",
    "section": "Carga de librerías",
    "text": "Carga de librerías\nLas librerías que utilizaremos en esta guía práctica son:\n\nlibrary(pacman)\npacman::p_load(tidyverse,   # manipulacion datos\n               sjPlot,      # tablas\n               confintr,    # IC\n               gginference, # visualizacion \n               rempsyc,     # reporte\n               broom,       # varios\n               sjmisc,      # para descriptivos\n               knitr)       # para       \n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls()) # para limpiar el entorno de trabajo"
  },
  {
    "objectID": "assignment/09-practico.html#carga-de-datos",
    "href": "assignment/09-practico.html#carga-de-datos",
    "title": "Reportes dinámicos 3",
    "section": "Carga de datos",
    "text": "Carga de datos\nComo señalamos antes, cargaremos los datos del Estudio Longitudinal Social de Chile (ELSOC) para ejemplificar. En esta guía práctica llamaremos los datos desde la web, pero en la carpeta del proyecto se encuentran alojados en la carpeta input/data, y se llama de la siguiente forma:\nload(\"input/data/ELSOC_Long.RData\")\nPero, para llamar la base de datos desde la web:\n\nload(url(\"https://dataverse.harvard.edu/api/access/datafile/7245118\"))"
  },
  {
    "objectID": "assignment/09-practico.html#limpieza-de-datos",
    "href": "assignment/09-practico.html#limpieza-de-datos",
    "title": "Reportes dinámicos 3",
    "section": "Limpieza de datos",
    "text": "Limpieza de datos\nRealizaremos un tratamiento simple de los casos:\n\n# Filtrar casos y seleccionar variables\ndata &lt;- elsoc_long_2016_2022.2 %&gt;% \n  filter(ola==1) %&gt;%\n  select(sexo=m0_sexo,edad=m0_edad,nedu=m01,\n         s11_01,s11_02,s11_03,s11_04,s11_05,s11_06,s11_07,s11_08,s11_09)\n\n# remover NA's\ndata &lt;- data %&gt;% \n  set_na(., na = c(-888, -999)) %&gt;% \n  na.omit()\n\n# crear variable nueva \ndata &lt;- data %&gt;% \n  rowwise() %&gt;%\n  mutate(sint_depresivos = mean(c(s11_01,s11_02,s11_03,s11_04,s11_05,s11_06,s11_07,s11_08,s11_09))) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "assignment/09-practico.html#guardar-base-de-datos-resultante",
    "href": "assignment/09-practico.html#guardar-base-de-datos-resultante",
    "title": "Reportes dinámicos 3",
    "section": "Guardar base de datos resultante",
    "text": "Guardar base de datos resultante\nGuardamos la base de datos procesada en la carpeta de output, con\nsaveRDS(data, \"output/data.Rdata\")"
  },
  {
    "objectID": "assignment/09-practico.html#introducción",
    "href": "assignment/09-practico.html#introducción",
    "title": "Reportes dinámicos 3",
    "section": "Introducción",
    "text": "Introducción\n\nEn este apartado pueden poner su introducción, de acuerdo con la pauta del trabajo que se encuentra disponible en el enlace.\n\nEn este ejemplo daremos una mirada a la salud mental, y exploraremos posibles asociaciones con la edad, sexo y nivel educacional."
  },
  {
    "objectID": "assignment/09-practico.html#variables",
    "href": "assignment/09-practico.html#variables",
    "title": "Reportes dinámicos 3",
    "section": "Variables",
    "text": "Variables\n\nEn este apartado pueden poner sus variables, de acuerdo con la pauta del trabajo que se encuentra disponible en el enlace.\n\nA continuación, en nuestro ejemplo describiremos las variables necesarias para responder a nuestro objetivo."
  },
  {
    "objectID": "assignment/09-practico.html#descripción-de-variables",
    "href": "assignment/09-practico.html#descripción-de-variables",
    "title": "Reportes dinámicos 3",
    "section": "Descripción de variables",
    "text": "Descripción de variables\nEn este ejemplo, se seleccionaron las variables:\n\nsexo: sexo del encuestado, con nivel de medición nominal\nedad: edad del encuestado, con nivel de medición intervalar\nnedu: nivel educativo del encuestado, con nivel de medición ordinal\n\nY las variables del módulo de Salud y Bienestar, referentes a Estado de ánimo: sintomatología depresiva, con nivel de medición ordinal, los ítems son los siguientes:\n\nFrecuencia: Poco interés o alegría\nFrecuencia: Decaimiento, pesadez o desesperanza\nFrecuencia: Dificultad para dormir o exceso de sueño\nFrecuencia: Cansancio o sensación de falta de energía\nFrecuencia: Apetito disminuido o aumentado\nFrecuencia: Dificultad para concentrarse\nFrecuencia: Mala opinión de sí mismo\nFrecuencia: Enlentecimiento físico\nFrecuencia: Pensamiento de muerte o dañarse\n\n\nPodemos caargar los datos en nuestro informe sin que se vea poniendo la option echo=FALSE: data &lt;-readRDS(\"output/data.RData\").\n\n\ntab1 &lt;- data %&gt;%\n  group_by(sexo) %&gt;% # agrupamos por sexo\n  summarise(n = n()) %&gt;% # contamos por categ de respuesta\n  mutate(prop = round((n / sum(n)) * 100, 2)) # porcentaje\n \npm &lt;- as.numeric(tab1[2,3])\nph &lt;- as.numeric(tab1[1,3])\n\ntabla1 &lt;- tab1 %&gt;% \n  kableExtra::kable(format = \"html\",\n                    align = \"c\",\n                    col.names = c(\"Sexo\", \"n\", \"Proporción\"),\n                    caption = \"Tabla 1. Distribución de sexo\") %&gt;% \n  kableExtra::kable_classic(full_width = FALSE, position = \"center\", font_size = 14) %&gt;% \n  kableExtra::add_footnote(label = \"Fuente: Elaboración propia en base a ELSOC 2016.\")\n\nEn la Tabla 1 podemos ver que la proporción de mujeres que responde la encuesta corresponde a 60.12%, mientras que la propoción de hombres corresponde a 39.88%.\n\ntabla1\n\n\n\nTabla 1: Distribución de sexo\n\n\n\n\nTabla 1. Distribución de sexo\n\n\nSexo\nn\nProporción\n\n\n\n\n1\n1151\n39.88\n\n\n2\n1735\n60.12\n\n\n\na Fuente: Elaboración propia en base a ELSOC 2016.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn las options del chunk desde el que llamamos a la tabla le ponemos tbl-nombre tabla para etiquetarla, y luego en el texto ponemos @tbl-nombre-tabla para referenciarla. También, cuando queremos llamar un valor, es necesario asignarlo a un objeto, y en el texto lo llamamos con:"
  },
  {
    "objectID": "assignment/09-practico.html#análisis",
    "href": "assignment/09-practico.html#análisis",
    "title": "Reportes dinámicos 3",
    "section": "Análisis",
    "text": "Análisis\n\nEn este apartado pueden poner sus análisis, de acuerdo con la pauta del trabajo que se encuentra disponible en el enlace.\n\nEn nuestro ejemplo, analizaremos la correlación entre algunas variables.\n\ncor_edad_dep &lt;- cor(data$edad, data$sint_depresivos)\ncor_nedu_dep &lt;- cor(data$nedu, data$sint_depresivos)\ncor_nedu_edad &lt;- cor(data$nedu, data$edad)\n\n\ng1 &lt;- data %&gt;% \n  group_by(nedu) %&gt;% \n  summarise(sint_dep = mean(sint_depresivos, na.rm = T),\n            edad=mean(edad, na.rm = T))\n\ngrafico1 &lt;- ggplot(data = g1,\n       mapping = aes(x = sint_dep, y = edad, label = nedu)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",colour = \"black\",fill=\"lightblue\",size=0.5) + \n  labs(x = \"Sintomatología depresiva\",\n       y = \"Edad\",\n       caption = \"Fuente: Elaboración propia en base a ELSOC 2026\") +\n  theme_bw()\n\nEn la Figura 1 es posible apreciar… La correlación entre la edad y el promedio de la sintomatología depresiva corresponde a 0.0176236.\n\ngrafico1\n\n\n\n\n\n\n\nFigura 1: Correlación entre edad y sintomatología depresiva\n\n\n\n\n\n\nEn las options del chunk desde el que llamamos a la figura le ponemos fig-nombre figura para etiquetarla, y luego en el texto ponemos @fig-nombre-figura para referenciarla."
  },
  {
    "objectID": "assignment/09-practico.html#conclusiones",
    "href": "assignment/09-practico.html#conclusiones",
    "title": "Reportes dinámicos 3",
    "section": "Conclusiones",
    "text": "Conclusiones\n\nEn este apartado pueden poner sus conclusiones, de acuerdo con la pauta del trabajo que se encuentra disponible en el enlace.\n\nAquí redactamos algunas conclusiones."
  },
  {
    "objectID": "assignment/09-practico.html#bibliografía",
    "href": "assignment/09-practico.html#bibliografía",
    "title": "Reportes dinámicos 3",
    "section": "Bibliografía",
    "text": "Bibliografía\n\nEn este apartado pueden poner su bibliografía, de acuerdo con la pauta del trabajo que se encuentra disponible en el enlace.\n\nPonemos un - para generar un listado.\n\nCOES (2023). Radiografía del Cambio Social: Análisis de Resultados Longitudinales ELSOC 2016-2022. Presentación de Resultados COES. Marzo, Santiago de Chile.\nR Core Team (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/."
  },
  {
    "objectID": "assignment/05-practico.html",
    "href": "assignment/05-practico.html",
    "title": "Reportes dinámicos 2",
    "section": "",
    "text": "El objetivo de esta guía práctica es aprender cómo crear y mostrar tablas y gráficos en documentos dinámicos mediante R Markdown. Además, aprenderemos cómo autoreferenciar elementos dentro de nuestro documento Rmd.\nEn detalle, aprenderemos:\n\nGeneración y presentación de tablas en R Markdown.\nGeneración y presentación de gráficos en R Markdown.\nCómo autoreferenciar elementos dentro un documento R Markdown.\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados derivados de las encuestas realizadas en diferentes países por el Latin American Public Opinion Proyect (LAPOP) en su ola del 2018. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  LAPOP 2018. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos LAPOP 2018."
  },
  {
    "objectID": "assignment/05-practico.html#recursos-de-la-práctica",
    "href": "assignment/05-practico.html#recursos-de-la-práctica",
    "title": "Reportes dinámicos 2",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados derivados de las encuestas realizadas en diferentes países por el Latin American Public Opinion Proyect (LAPOP) en su ola del 2018. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  LAPOP 2018. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos LAPOP 2018."
  },
  {
    "objectID": "assignment/05-practico.html#chunks",
    "href": "assignment/05-practico.html#chunks",
    "title": "Reportes dinámicos 2",
    "section": "Chunks",
    "text": "Chunks\nPara integrar código de R en un archivo RMarkdown usamos los chunks, que son trozos de código dentro de nuestra hoja. Estos permiten hacer análisis dentro del documento visualizando los resultados en el documento final.\nLos chunks se ven así dentro del .Rmd:\n```{r}\n# El codigo va aquí\n\n```\n\nInsertar chunks\nHay tres formas de insertar chunks:\n\nPulsar ⌘⌥I en macOS o Control + Alt + I en Windows\nPulsa el botón “Insert” en la parte superior de la ventana del editor\n\n\n\n\n\n\n\n\n\n\n\nEscribirlo manualmente (no recomendado)\n\n\n\nNombre de chunk\nPara añadir un nombre, inclúyelo inmediatamente después de la {r en la primera línea del chunk. Los nombres no pueden contener espacios, pero sí guiones bajos y guiones.\nImportante: Todos los nombres de chunk de tu documento deben ser únicos.\n```{r nombre-chunk}\n# El codigo va aquí\n\n```\n\n\nOpciones de chunk\nHay distintas opciones diferentes que puedes establecer para cada chunk. Puedes ver una lista completa en la Guía de referencia de RMarkdown o en el sitio web de knitr.\nLas opciones van dentro de la sección {r} del chunk:\n```{r nombre-chunk, message = FALSE, echo = TRUE}\n# El codigo va aquí\n\n```\nOtra forma de hacerlo es configurar las opciones generales de todos los chunks que hagamos al inicio del documento:\n```{r setup, include = FALSE}\nknitr::opts_chunk$set(echo = TRUE, \n                      message = FALSE, \n                      warning = FALSE)\n\n```\nDe esta manera ya no es necesario indicar en cada chunk las opciones, y se aplicaran las configuraciones generales que indicamos al comienzo."
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html",
    "href": "files/reporte_notas/reporte_notas.html",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "",
    "text": "pacman::p_load(googlesheets4, dplyr, sjmisc,sjPlot, kableExtra, sjlabelled )\n\n\n\n [1] \"N\"            \"Persona\"      \"p1-a\"         \"p1-b\"         \"p2\"          \n [6] \"p3\"           \"p4\"           \"p5\"           \"pje\"          \"Nota 60%\"    \n[11] \"corrector/a\"  \"decimas\"      \"nota_final\"   \"Recorrección\" \"...15\"       \n[16] \"...16\"       \n\n\n\nload(\"prueba1.Rdata\")\n\n\n\n\n\n# Label variables\n\nprueba1$p1a &lt;- set_label(x = prueba1$p1a, \n                         label = \"Cálculo Pearson\")\nprueba1$p1b &lt;- set_label(x = prueba1$p1b, \n                         label = \"Interpretación Pearson\")\nprueba1$p2 &lt;- set_label(x = prueba1$p2, \n                         label = \"Limitación Pearson\")\nprueba1$p3 &lt;- set_label(x = prueba1$p3, \n                         label = \"Coeficiente determinación\")\nprueba1$p4 &lt;- set_label(x = prueba1$p4, \n                         label = \"Spearman\")\nprueba1$p5 &lt;- set_label(x = prueba1$p5, \n                         label = \"Perdidos matrices\")\n\n\n\n\n\nprueba1 %&gt;% descr(., show = c(\"label\",\"range\", \"mean\", \"sd\", \"n\"))%&gt;% kable(.,\"markdown\", digits=2)\n\n\n\n\n\nvar\nlabel\nn\nmean\nsd\nrange\n\n\n\n\n2\np1a\nCálculo Pearson\n79\n1.78\n1.29\n3 (0-3)\n\n\n3\np1b\nInterpretación Pearson\n79\n1.66\n0.78\n3 (0-3)\n\n\n4\np2\nLimitación Pearson\n79\n1.53\n0.69\n2 (0-2)\n\n\n5\np3\nCoeficiente determinación\n79\n1.50\n0.64\n2 (0-2)\n\n\n6\np4\nSpearman\n79\n1.39\n0.81\n2 (0-2)\n\n\n7\np5\nPerdidos matrices\n79\n0.99\n0.89\n2 (0-2)\n\n\n1\nnota_final\nnota_final\n79\n4.54\n1.40\n6 (1-7)\n\n\n\n\n\n\n\n\n\nhist(prueba1$nota_final)\n\n\n\n\n\n\n\nplot_frq(data = prueba1$nota_final,type = \"hist\",show.mean = T)\n\n\n\n\n\n\n\n\n\nprueba1 &lt;- prueba1 %&gt;%  mutate(notas_cat=cut(nota_final, breaks=c(-Inf,4,5,6, Inf), labels=c(\"Menor a 4.0\",\"4.0-5.0\",\"5.0-6.0\",\"6.0-7.0\")))\n\nfrq(prueba1$notas_cat)\n\nx &lt;categorical&gt; \n# total N=88 valid N=79 mean=2.23 sd=1.13\n\nValue       |  N | Raw % | Valid % | Cum. %\n-------------------------------------------\nMenor a 4.0 | 30 | 34.09 |   37.97 |  37.97\n4.0-5.0     | 14 | 15.91 |   17.72 |  55.70\n5.0-6.0     | 22 | 25.00 |   27.85 |  83.54\n6.0-7.0     | 13 | 14.77 |   16.46 | 100.00\n&lt;NA&gt;        |  9 | 10.23 |    &lt;NA&gt; |   &lt;NA&gt;\n\nprueba1 &lt;- prueba1 %&gt;% dplyr::select(-notas_cat)\n\n\n\n\n\ntab_corr(prueba1,\n         triangle = \"lower\")\n\n\n\n\n \nCálculo Pearson\nInterpretación Pearson\nLimitación Pearson\nCoeficiente determinación\nSpearman\nPerdidos matrices\nnota_final\n\n\nCálculo Pearson\n \n \n \n \n \n \n \n\n\nInterpretación Pearson\n0.405***\n \n \n \n \n \n \n\n\nLimitación Pearson\n0.211\n0.153\n \n \n \n \n \n\n\nCoeficiente determinación\n0.271*\n0.407***\n0.267*\n \n \n \n \n\n\nSpearman\n0.229*\n0.203\n0.375***\n0.197\n \n \n \n\n\nPerdidos matrices\n0.063\n0.127\n0.435***\n0.095\n0.277*\n \n \n\n\nnota_final\n0.696***\n0.627***\n0.619***\n0.524***\n0.583***\n0.539***\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\n\nplot_scatter(prueba1, p1a, p1b)\n\n\n\n\n\n\n\n\nConsistencia interna\n\npacman::p_load(ltm)\n\npreguntas &lt;- prueba1 %&gt;% dplyr::select(-nota_final)\ncronbach.alpha(na.omit(preguntas))\n\n\nCronbach's alpha for the 'na.omit(preguntas)' data-set\n\nItems: 6\nSample units: 79\nalpha: 0.634"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#librerías-datos",
    "href": "files/reporte_notas/reporte_notas.html#librerías-datos",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "",
    "text": "pacman::p_load(googlesheets4, dplyr, sjmisc,sjPlot, kableExtra, sjlabelled )\n\n\n\n [1] \"N\"            \"Persona\"      \"p1-a\"         \"p1-b\"         \"p2\"          \n [6] \"p3\"           \"p4\"           \"p5\"           \"pje\"          \"Nota 60%\"    \n[11] \"corrector/a\"  \"decimas\"      \"nota_final\"   \"Recorrección\" \"...15\"       \n[16] \"...16\"       \n\n\n\nload(\"prueba1.Rdata\")"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#etiquetados",
    "href": "files/reporte_notas/reporte_notas.html#etiquetados",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "",
    "text": "# Label variables\n\nprueba1$p1a &lt;- set_label(x = prueba1$p1a, \n                         label = \"Cálculo Pearson\")\nprueba1$p1b &lt;- set_label(x = prueba1$p1b, \n                         label = \"Interpretación Pearson\")\nprueba1$p2 &lt;- set_label(x = prueba1$p2, \n                         label = \"Limitación Pearson\")\nprueba1$p3 &lt;- set_label(x = prueba1$p3, \n                         label = \"Coeficiente determinación\")\nprueba1$p4 &lt;- set_label(x = prueba1$p4, \n                         label = \"Spearman\")\nprueba1$p5 &lt;- set_label(x = prueba1$p5, \n                         label = \"Perdidos matrices\")"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#tabla-descriptiva",
    "href": "files/reporte_notas/reporte_notas.html#tabla-descriptiva",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "",
    "text": "prueba1 %&gt;% descr(., show = c(\"label\",\"range\", \"mean\", \"sd\", \"n\"))%&gt;% kable(.,\"markdown\", digits=2)\n\n\n\n\n\nvar\nlabel\nn\nmean\nsd\nrange\n\n\n\n\n2\np1a\nCálculo Pearson\n79\n1.78\n1.29\n3 (0-3)\n\n\n3\np1b\nInterpretación Pearson\n79\n1.66\n0.78\n3 (0-3)\n\n\n4\np2\nLimitación Pearson\n79\n1.53\n0.69\n2 (0-2)\n\n\n5\np3\nCoeficiente determinación\n79\n1.50\n0.64\n2 (0-2)\n\n\n6\np4\nSpearman\n79\n1.39\n0.81\n2 (0-2)\n\n\n7\np5\nPerdidos matrices\n79\n0.99\n0.89\n2 (0-2)\n\n\n1\nnota_final\nnota_final\n79\n4.54\n1.40\n6 (1-7)"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#gráficos-descriptivos",
    "href": "files/reporte_notas/reporte_notas.html#gráficos-descriptivos",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "",
    "text": "hist(prueba1$nota_final)\n\n\n\n\n\n\n\nplot_frq(data = prueba1$nota_final,type = \"hist\",show.mean = T)\n\n\n\n\n\n\n\n\n\nprueba1 &lt;- prueba1 %&gt;%  mutate(notas_cat=cut(nota_final, breaks=c(-Inf,4,5,6, Inf), labels=c(\"Menor a 4.0\",\"4.0-5.0\",\"5.0-6.0\",\"6.0-7.0\")))\n\nfrq(prueba1$notas_cat)\n\nx &lt;categorical&gt; \n# total N=88 valid N=79 mean=2.23 sd=1.13\n\nValue       |  N | Raw % | Valid % | Cum. %\n-------------------------------------------\nMenor a 4.0 | 30 | 34.09 |   37.97 |  37.97\n4.0-5.0     | 14 | 15.91 |   17.72 |  55.70\n5.0-6.0     | 22 | 25.00 |   27.85 |  83.54\n6.0-7.0     | 13 | 14.77 |   16.46 | 100.00\n&lt;NA&gt;        |  9 | 10.23 |    &lt;NA&gt; |   &lt;NA&gt;\n\nprueba1 &lt;- prueba1 %&gt;% dplyr::select(-notas_cat)"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#preguntas-y-asociaciones",
    "href": "files/reporte_notas/reporte_notas.html#preguntas-y-asociaciones",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "",
    "text": "tab_corr(prueba1,\n         triangle = \"lower\")\n\n\n\n\n \nCálculo Pearson\nInterpretación Pearson\nLimitación Pearson\nCoeficiente determinación\nSpearman\nPerdidos matrices\nnota_final\n\n\nCálculo Pearson\n \n \n \n \n \n \n \n\n\nInterpretación Pearson\n0.405***\n \n \n \n \n \n \n\n\nLimitación Pearson\n0.211\n0.153\n \n \n \n \n \n\n\nCoeficiente determinación\n0.271*\n0.407***\n0.267*\n \n \n \n \n\n\nSpearman\n0.229*\n0.203\n0.375***\n0.197\n \n \n \n\n\nPerdidos matrices\n0.063\n0.127\n0.435***\n0.095\n0.277*\n \n \n\n\nnota_final\n0.696***\n0.627***\n0.619***\n0.524***\n0.583***\n0.539***\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\n\nplot_scatter(prueba1, p1a, p1b)\n\n\n\n\n\n\n\n\nConsistencia interna\n\npacman::p_load(ltm)\n\npreguntas &lt;- prueba1 %&gt;% dplyr::select(-nota_final)\ncronbach.alpha(na.omit(preguntas))\n\n\nCronbach's alpha for the 'na.omit(preguntas)' data-set\n\nItems: 6\nSample units: 79\nalpha: 0.634"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#datos",
    "href": "files/reporte_notas/reporte_notas.html#datos",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "Datos",
    "text": "Datos\n\nload(\"prueba2.Rdata\")"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#adecuación-preguntas",
    "href": "files/reporte_notas/reporte_notas.html#adecuación-preguntas",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "Adecuación preguntas",
    "text": "Adecuación preguntas\nLa prueba tiene dos formas: A y B, la única diferencia es en el orden de las preguntas, la forma A parte por la pregunta sobre identificación con la izquierda, mientras que en la forma B la primera pregunta es la de CASEN. Para poder realizar un correcto análisis de las preguntas vamos a adaptar la forma B a la A.\n\nfrq(prueba2$forma)\n\nx &lt;character&gt; \n# total N=87 valid N=72 mean=1.50 sd=0.50\n\nValue |  N | Raw % | Valid % | Cum. %\n-------------------------------------\nA     | 36 | 41.38 |      50 |     50\nB     | 36 | 41.38 |      50 |    100\n&lt;NA&gt;  | 15 | 17.24 |    &lt;NA&gt; |   &lt;NA&gt;\n\nA &lt;- subset(prueba2, subset = prueba2$forma==\"A\")\nB &lt;- subset(prueba2, subset = prueba2$forma==\"B\")\ndim(A)\n\n[1] 36  8\n\ndim(B)\n\n[1] 36  8\n\ndescr(B, show = \"mean\")\n\n\n## Basic descriptive statistics\n\n        var mean\n        p1a 1.33\n        p1b 1.36\n        p1c 1.31\n        p2a 1.39\n        p2b 1.28\n        p2c 1.32\n nota_final 4.58\n\nB &lt;- B %&gt;% rename(\"p1a\"=\"p2a\", \n                  \"p1b\"=\"p2b\",\n                  \"p1c\"=\"p2c\",\n                  \"p2a\"=\"p1a\",\n                  \"p2b\"=\"p1b\",\n                  \"p2c\"=\"p1c\")\n\ndescr(B, show = \"mean\")\n\n\n## Basic descriptive statistics\n\n        var mean\n        p2a 1.33\n        p2b 1.36\n        p2c 1.31\n        p1a 1.39\n        p1b 1.28\n        p1c 1.32\n nota_final 4.58\n\nprueba2 &lt;- rbind(A,B)"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#etiquetados-1",
    "href": "files/reporte_notas/reporte_notas.html#etiquetados-1",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "Etiquetados",
    "text": "Etiquetados\n\n# Label variables\n\nprueba2$p1a &lt;- set_label(x = prueba2$p1a, \n                         label = \"H1 promedio direccional\")\nprueba2$p1b &lt;- set_label(x = prueba2$p1b, \n                         label = \"Prueba promedios\")\nprueba2$p1c &lt;- set_label(x = prueba2$p1c, \n                         label = \"Rechazo H0\")\nprueba2$p2a &lt;- set_label(x = prueba2$p2a, \n                         label = \"H1 proporcion no direccional\")\nprueba2$p2b &lt;- set_label(x = prueba2$p2b, \n                         label = \"Error tipo 1\")\nprueba2$p2c &lt;- set_label(x = prueba2$p2c, \n                         label = \"Intervalo confianza\")"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#tabla-descriptiva-1",
    "href": "files/reporte_notas/reporte_notas.html#tabla-descriptiva-1",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "Tabla descriptiva",
    "text": "Tabla descriptiva\n\nprueba2 %&gt;% descr(., show = c(\"label\",\"range\", \"mean\", \"sd\", \"n\"))%&gt;% kable(.,\"markdown\", digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar\nlabel\nn\nmean\nsd\nrange\n\n\n\n\n2\np1a\nH1 promedio direccional\n72\n1.36\n0.63\n2 (0-2)\n\n\n3\np1b\nPrueba promedios\n72\n1.24\n0.86\n2 (0-2)\n\n\n4\np1c\nRechazo H0\n72\n1.27\n0.70\n2 (0-2)\n\n\n5\np2a\nH1 proporcion no direccional\n72\n1.26\n0.68\n2 (0-2)\n\n\n6\np2b\nError tipo 1\n72\n1.27\n0.79\n2 (0-2)\n\n\n7\np2c\nIntervalo confianza\n72\n1.14\n0.72\n2 (0-2)\n\n\n1\nnota_final\nnota_final\n72\n4.37\n1.23\n5.6 (1.4-7)"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#gráficos-descriptivos-1",
    "href": "files/reporte_notas/reporte_notas.html#gráficos-descriptivos-1",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "Gráficos descriptivos",
    "text": "Gráficos descriptivos\n\nhist(prueba2$nota_final)\n\n\n\n\n\n\n\nplot_frq(data = prueba2$nota_final,type = \"hist\",show.mean = T)\n\n\n\n\n\n\n\n\n\nprueba2 &lt;- prueba2 %&gt;%  mutate(notas_cat=cut(nota_final, breaks=c(-Inf,4,5,6, Inf), labels=c(\"Menor a 4.0\",\"4.0-5.0\",\"5.0-6.0\",\"6.0-7.0\")))\n\nfrq(prueba2$notas_cat)\n\nx &lt;categorical&gt; \n# total N=72 valid N=72 mean=2.06 sd=1.02\n\nValue       |  N | Raw % | Valid % | Cum. %\n-------------------------------------------\nMenor a 4.0 | 28 | 38.89 |   38.89 |  38.89\n4.0-5.0     | 19 | 26.39 |   26.39 |  65.28\n5.0-6.0     | 18 | 25.00 |   25.00 |  90.28\n6.0-7.0     |  7 |  9.72 |    9.72 | 100.00\n&lt;NA&gt;        |  0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nprueba2 &lt;- prueba2 %&gt;% dplyr::select(-notas_cat)"
  },
  {
    "objectID": "files/reporte_notas/reporte_notas.html#preguntas-y-asociaciones-1",
    "href": "files/reporte_notas/reporte_notas.html#preguntas-y-asociaciones-1",
    "title": "Reporte Notas - Estadística Correlacional 2023",
    "section": "Preguntas y asociaciones",
    "text": "Preguntas y asociaciones\n\nprueba2 &lt;- prueba2 %&gt;% dplyr::select(-forma)\ntab_corr(prueba2,\n         triangle = \"lower\")\n\n\n\n\n \nH1 promedio direccional\nPrueba promedios\nRechazo H0\nH1 proporcion no direccional\nError tipo 1\nIntervalo confianza\nnota_final\n\n\nH1 promedio direccional\n \n \n \n \n \n \n \n\n\nPrueba promedios\n0.295*\n \n \n \n \n \n \n\n\nRechazo H0\n0.397***\n0.488***\n \n \n \n \n \n\n\nH1 proporcion no direccional\n0.108\n0.094\n-0.060\n \n \n \n \n\n\nError tipo 1\n-0.204\n0.039\n0.031\n-0.039\n \n \n \n\n\nIntervalo confianza\n0.047\n0.206\n0.308**\n0.169\n0.351**\n \n \n\n\nnota_final\n0.469***\n0.677***\n0.650***\n0.370**\n0.404***\n0.631***\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nConsistencia interna\n\npacman::p_load(ltm)\n\npreguntas &lt;- prueba2 %&gt;% dplyr::select(-nota_final)\ncronbach.alpha(na.omit(preguntas))\n\n\nCronbach's alpha for the 'na.omit(preguntas)' data-set\n\nItems: 6\nSample units: 72\nalpha: 0.513"
  },
  {
    "objectID": "assignment/04-practico.html",
    "href": "assignment/04-practico.html",
    "title": "Inferencia 1: Curva normal",
    "section": "",
    "text": "Objetivo de la práctica\nEl objetivo de esta guía práctica es introducirnos en la inferencia estadística, revisando los conceptos y aplicaciones de la curva normal y las probabilidades bajo esta con puntajes Z.\nEn detalle, aprenderemos:\n\nQué es la inferencia estadística.\nQué es la distribución normal y cómo interpretarla.\nCómo calcular probabilidades asociadas con valores Z en R.\n\n\n\n¿Qué es la inferencia estadística?\nEn estadística, llamamos inferencia al ejercicio de extrapolar determinadas estimaciones (estadístico) de una muestra a una población más grande (parámetro). En concreto, es el proceso de realizar conclusiones o predicciones sobre una población a partir de una muestra o subconjunto de esa población.\n\nUn concepto central en todo esto es la probabilidad de error, es decir, en qué medida nos estamos equivocando (o estamos dispuestos a estar equivocados) en tratar de extrapolar una estimación muestral a la población.\n\n\nDistribución normal\n\n\n\n\n\n\nNota\n\n\n\nRecordemos que por distribución nos referimos al conjunto de todos los valores posibles de una variable y las frecuencias (o probabilidades) con las que se producen.\n\n\nExisten distribuciones empíricas y distribuciones teóricas, en donde:\n\nlas primeras reflejan la distribución de los valores que asume la variable en un grupo concreto a partir de una observación.\nlas segundas son una función matématica que expresan la distribución de un conjunto de números mediante su probabilidad de ocurencia.\n\nPara empezar, veamos una de las distribuciones teóricas más conocidas: la distribución normal estándar. La distribución normal estándar:\n\nes una distribución normal con una media de 0 y una desviación estándar de 1.\nsimétricas y con un solo punto de elevación\nla media se sitúa al centro, y la desviación estandar expresa su dispersión\nla pendiente es más fuerte cerca del centro, y se suaviza hacia los extremos\nlos puntos en los que tiene lugar este cambio de curvatura se hallan a una distancia σ, a ambos lados de la media µ.\n\nCon R es posible generar un conjunto de datos simulados con una distribución normal.\n\nx.values &lt;- seq(-4,4, length = 1000)\ny.values &lt;- dnorm(x.values)\nplot(x.values, y.values, type=\"l\", xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\")\n\n\n\n\n\n\n\n\n¿Qué estamos haciendo en cada una de las 3 líneas de código? ¿Qué variables se crearon y cómo nos aseguramos de que los datos generados siguieran una distribución normal? Pensemos un poco…\nAhora podemos preguntar qué parte de la curva cae por debajo de un valor particular. Por ejemplo, preguntaremos sobre el valor 0 antes de ejecutar el código. Piense ¿cuál debería ser la respuesta?\n\npnorm(q = 0)\n\n[1] 0.5\n\n\nTenemos que la probabilidad (en una curva normal estándar) de obtener un valor igual o menor a 0 es de 0.5, es decir, del 50%, pero ¿por qué?\n\nPorque como la distribución normal es simétrica alrededor de cero, la probabilidad de que sea menor o igual a cero es 0.5, es decir, el 50% de la distribución está por debajo de cero y el otro 50% está por encima de cero.\n\n\n\n\n:scale 65%\n\n\nEsto es posible mediante la relación entre las áreas bajo la curva normal y las probabilidades.\n\n\nProbabilidades asociadas con valores z\n\n\n\n\n\n\nNota\n\n\n\nLa puntuación Z es una medida que se utiliza para expresar la posición relativa de un valor con respecto a la media en una distribución normal. La puntuación Z mide cuántas desviaciones estándar está un valor por encima o por debajo de la media.\n\n\nEn los ejemplos siguientes, usaremos valores Z de + 1,96 y -1,96 porque sabemos que estos valores aproximados marcan el 2,5% superior e inferior de la distribución normal estándar. Esto corresponde a un alfa típico = 0,05 para una prueba de hipótesis de dos colas (sobre la cual aprenderemos más en las próximas semanas).\n\npnorm(q = 1.96, lower.tail=TRUE)\n\n[1] 0.9750021\n\n\nLa respuesta nos dice lo que ya sabemos: el 97,5% de la distribución normal ocurre por debajo del valor z de 1,96.\nPodemos agregar una línea al gráfico para mostrar dónde se usaría abline.\nEl 97,5% de la distribución queda por debajo de esta línea.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = 1.96)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\n¿Y si lo hacemos hacia la cola izquierda o inferior de la distribución?\n\npnorm(q = -1.96, lower.tail = TRUE)\n\n[1] 0.0249979\n\n\nTenemos que, hacia el extremo inferior de la distribución, el valor z -1,96 marca el 2,5% inferior de la distribución normal estándar.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = -1.96)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\n\nEjercicio 1\nUtilice la función abline() para agregar líneas en el puntaje z apropiado para demostrar el clásico 68-95-99.7 de esta curva normal estándar.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = 1) +\nabline(v = -1) +\nabline(v = 2) +\nabline(v = -2) +\nabline(v = 3) +\nabline(v = -3)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\nComo se discutió en clases, también podemos hacer lo contrario: decidir primero cuánta probabilidad queremos (percentil) y luego calcular qué valores críticos están asociados con esas probabilidades. Esto utiliza la función qnorm. Si queremos saber qué valor z marca la probabilidad p del 2,5% inferior de una distribución normal estándar, usaríamos:\n\nqnorm(p = 0.025)\n\n[1] -1.959964\n\n\nEsto nos dice que el valor z de -1,96 marca el 2,5% inferior de la distribución normal estándar. Para determinar el valor z que marca el 2,5% superior de la distribución, escribo:\n\nqnorm(p = 0.975)\n\n[1] 1.959964\n\n\n\n\nEjercicio 2\nHasta ahora hemos demostrado todo con una distribución normal estándar. Pero la mayoría de las curvas normales no son normales estándar.\nGenere una curva (como hicimos anteriormente para la distribución normal estándar) y trácela con una media de 20 y una desviación estándar de 1,65.\n\nx.values &lt;- seq(10,30, length = 1000)\ny.values &lt;- dnorm(x.values, mean = 20, sd = 1.65) # indico media y sd\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\")\n\n\n\n\n\n\n\n\nAhora, identifique el valor en el que el 97,5% de la distribución cae por debajo de este valor. Esto lo hicimos antes con qnorm.\n\nqnorm(p = .975, mean = 20, sd = 1.65)\n\n[1] 23.23394\n\n\nTenemos que el 97,5% de los valores estarán por debajo de 23,2.\n\n\n\nEjercicio de aplicación\nAhora que hemos generado distribuciones normales, echemos un vistazo a algunos datos y compárelos con la distribución normal. Utilizaremos un conjunto de datos desde internet, con mediciones de 247 hombres y 260 mujeres, la mayoría de los cuales eran considerados adultos jóvenes sanos.P uede encontrar una clave para los nombres de las variables aquí, pero nos centraremos en solo tres columnas: peso en kg (wgt), altura en cm (hgt) y sexo (1 = hombre; 0 = mujer).\n\nload(url(\"http://www.openintro.org/stat/data/bdims.RData\"))\n\nSeparemos estos datos en dos conjuntos, uno de hombres y otro de mujeres con la función subset\n\nmdims &lt;- subset(bdims, sex == 1)\nfdims &lt;- subset(bdims, sex == 0)\n\n\nEjercicio 1\nHaz un histograma de la altura de los hombres y un histograma de la altura de las mujeres. ¿Cómo compararía los diversos aspectos de las dos distribuciones?\n\nhist(mdims$hgt, xlim = c(150,200))\n\n\n\n\n\n\n\nhist(fdims$hgt, xlim = c(140,190))\n\n\n\n\n\n\n\n\n\n\nEjercicio 2\nscale es una función en R y se puede aplicar a cualquier vector numérico (lista de números en R). Genere los dos histogramas siguientes, esta vez graficando scale() de las estaturas y determine cómo la versión escalada de las alturas corresponde a las alturas originales. ¿Qué calcula la escala para cada punto?\n\nhist(scale(mdims$hgt))\n\n\n\n\n\n\n\nhist(scale(fdims$hgt))\n\n\n\n\n\n\n\n\n\n\nEjercicio 3\nNos gustaría comparar la distribución de estaturas en este conjunto de datos con la distribución normal. Para cada uno de los histogramas de alturas (sin escalar), trace una curva normal en la parte superior del histograma.\n\nCalcule la media y la desviación estándar para las alturas femeninas y guárdelas como variables, fhgtmean y fhgtsd, respectivamente.\nDetermine la lista de valores de x (el rango del eje X) y guarde este vector. Puede hacer fácilmente una lista de números usando la función seq() como lo hemos hecho antes, o teniendo el límite inferior:límite superior. Por ejemplo, para generar un vector (lista de números) del 1 al 10 y guardarlo como one_ten, usaría one_ten &lt;- 1:10.\nComo arriba, use dnorm() para tomar la lista de valores de x y encontrar el valor de y correspondiente si fuera una distribución normal perfecta. Guarde este vector como la variable y.\nVuelva a trazar su histograma y luego, en la siguiente línea, use lines(x = x, y = y, col = \"blue\") para dibujar una distribución normal encima.\n\n\nfhgtmean &lt;- mean(fdims$hgt)\nfhgtsd   &lt;- sd(fdims$hgt)\nhist(fdims$hgt, probability = TRUE, ylim = c(0, .07))\nx &lt;- 140:190\ny &lt;- dnorm(x = x, mean = fhgtmean, sd = fhgtsd)\nlines(x = x, y = y, col = \"blue\")\n\n\n\n\n\n\n\n\nSegún este gráfico, ¿parece que los datos siguen una distribución casi normal? Haz lo mismo con las estaturas masculinas.\n\nRespuesta: En general, sí, consideraría que estos valores siguen una distribución casi normal ya que el histograma se ajusta bastante bien a la curva.\n\nObserve que la forma del histograma es una forma de determinar si los datos parecen estar distribuidos casi normalmente, pero puede resultar frustrante decidir qué tan cerca está el histograma de la curva. Un enfoque alternativo implica construir una gráfica de probabilidad normal, también llamada gráfica Q-Q por “quantil-quantil”. Ejecute ambas líneas juntas.\n\nqqnorm(fdims$hgt)\nqqline(fdims$hgt)\n\n\n\n\n\n\n\n\nUn QQ plot nos muestra en el eje x los cuantiles teóricos de la distribución en términos de desviaciones estandar, y en el eje y los valores de la variable. La distribución de los puntos en una línea recta es una indicación de que los datos se distribuyen normalmente.\nVeamos otro ejemplo de otra variable de la base de datos:\n\nhist(fdims$che.de)\n\n\n\n\n\n\n\nqqnorm(fdims$che.de)\nqqline(fdims$che.de)\n\n\n\n\n\n\n\n\nUna vez que decidimos que una variable se distribuyte de forma normal, podemos responder todo tipo de preguntas sobre esa variable relacionadas con la probabilidad. Tomemos, por ejemplo, la pregunta: “¿Cuál es la probabilidad de que una mujer adulta joven elegida al azar mida más 182 cm?”\nSi suponemos que las alturas de las mujeres se distribuyen normalmente (una aproximación muy cercana también está bien), podemos encontrar esta probabilidad calculando una puntuación Z y consultando una tabla Z (también llamada tabla de probabilidad normal).\nEn R, esto se hace en un solo paso con la función pnorm (como hicimos anteriormente para la distribución normal estándar).\n\npnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.9955656\n\n\nObtenemos la proporción de mujeres que está bajo esa estatura, es decir 99,6%. Si queremos saber la proporción de mujeres que está sobre esa estatura:\n\n1 - pnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.004434387\n\n\nEn este caso, el 0,4% de las mujeres se encontraría sobre esa estatura.\nPodemos también hacer la operación inversa, es decir, a qué valor (estatura) corresponde un porcentaje o probabilidad basada en una distribución normal. Para ello utilizamos la función qnorm. Por ejemplo, para la probabilidad que calculamos más arriba para una altura de 182cm en las mujeres:\n\nqnorm(.9955656, fhgtmean, fhgtsd)\n\n[1] 182\n\n\n\n\n\nReporte de progreso\nCompletar el reporte de progreso correspondiente a esta práctica aquí. El plazo para contestarlo es hasta el día viernes de la semana en la que se publica la práctica correspondiente.\n\n\nVideo de práctica\n\n\n\nForo"
  },
  {
    "objectID": "index.html#últimas-informaciones",
    "href": "index.html#últimas-informaciones",
    "title": "\n            Estadística Correlacional\n        ",
    "section": "Últimas informaciones",
    "text": "Últimas informaciones\n\n\n\n\n\n\n\n\n\n\nSesión práctica martes 27 de agosto\n\n\n\n\n\n\n\n\n\nAug 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrácticos parten próxima semana (Martes 27)\n\n\n\n\n\n\n\n\n\nAug 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformaciones por acá\n\n\n\n\n\n\n\n\n\nAug 19, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n-&gt; ir a página de Informaciones"
  },
  {
    "objectID": "index.html#versiones-previas-del-curso",
    "href": "index.html#versiones-previas-del-curso",
    "title": "\n            Estadística Correlacional\n        ",
    "section": "Versiones previas del curso:",
    "text": "Versiones previas del curso:\n\n2023"
  },
  {
    "objectID": "news/2024-08-23_primera-sesion-practica.html",
    "href": "news/2024-08-23_primera-sesion-practica.html",
    "title": "Sesión práctica martes 27 de agosto",
    "section": "",
    "text": "← News\n\n\n\n\nAlgunas informaciones importantes:\n\nEl próximo martes 27 de agosto tendremos nuestra primera sesión práctica del curso.\nEn esta instancia contaremos con el equipo docente completo y ayudantes.\nAquellas personas que, por diversos motivos, no puedan contar con un computador personal para las sesiones del curso, les pedimos encarecidamente que rellenen este formulario: https://forms.gle/iGVJjAWRGYh9GGpZ8 Esta información nos permitirá ir organizando el uso de computadores de la sala de computación.\nNos dividiremos en dos grupos: una parte del curso en las salas C7 y C8 del aulario C (idealmente quienes dispongan de computador) y otra parte en la sala 345 de computación del edificio nuevo de Facso (para quienes no puedan contar con un computador).\nDentro de la próxima semana les enviaremos un correo respecto a la asignación de ayudantes para el asesoramiento en las unidades 1 y 2 del curso."
  },
  {
    "objectID": "news/2024-08-23_primera-practica.html",
    "href": "news/2024-08-23_primera-practica.html",
    "title": "Sesión práctica martes 27 de agosto",
    "section": "",
    "text": "← News\n\n\n\nAlgunas informaciones importantes:\n-El próximo martes 27 de agosto tendremos nuestra primera sesión práctica del curso.\n-En esta instancia contaremos con el equipo docente completo y ayudantes.\n-Aquellas personas que, por diversos motivos, no puedan contar con un computador personal para las sesiones del curso, les pedimos encarecidamente que rellenen este formulario: https://forms.gle/iGVJjAWRGYh9GGpZ8 Esta información nos permitirá ir organizando el uso de computadores de la sala de computación.\n-Nos dividiremos en dos grupos: una parte del curso en las salas C7 y C8 del aulario C (idealmente quienes dispongan de computador) y otra parte en la sala 345 de computación del edificio nuevo de Facso (para quienes no puedan contar con un computador).\n-Dentro de la próxima semana les enviaremos un correo respecto a la asignación de ayudantes para el asesoramiento en las unidades 1 y 2 del curso."
  },
  {
    "objectID": "assignment/01-practico.html#librerías",
    "href": "assignment/01-practico.html#librerías",
    "title": "Distribución Normal e Intervalos de Confianza",
    "section": "",
    "text": "Cargaremos algunas librerías que serán necesarias en las diferentes partes de esta guía práctica:\n\nlibrary(pacman)\npacman::p_load(tidyverse, # para sintaxis\n               ggplot2,   # para gráficos\n               car,       # para recodificar\n               psych,     # para analizar datos\n               sjmisc,    # para analizar datos\n               srvyr,     # para estimación de IC y ponderadores\n               Publish)   # para IC)    \n\noptions(scipen = 999) # para desactivar notacion cientifica\nrm(list = ls())       # para limpar el entonrno de trabajo"
  },
  {
    "objectID": "assignment/01-practico.html#curvas-de-distribución",
    "href": "assignment/01-practico.html#curvas-de-distribución",
    "title": "Distribución Normal e Intervalos de Confianza",
    "section": "3.1. Curvas de distribución",
    "text": "3.1. Curvas de distribución\nPor distribución nos referimos al conjunto de todos los valores posibles de una variable y las frecuencias (o probabilidades) con las que se producen.\nExisten distribuciones empíricas y distribuciones teóricas, en donde:\n\nlas primeras reflejan la distribución de los valores que asume la variable en un grupo concreto a partir de una observación.\nlas segundas son una función matématica que expresan la distribución de un conjunto de números mediante su probabilidad de ocurencia.\n\nEstas últimas son también llamadas curvas de distribución."
  },
  {
    "objectID": "assignment/01-practico.html#distribución-normal-1",
    "href": "assignment/01-practico.html#distribución-normal-1",
    "title": "Distribución Normal e Intervalos de Confianza",
    "section": "3.2. Distribución Normal",
    "text": "3.2. Distribución Normal\nEs una distribución teórica que corresponde a una curva que representa la distribución de los casos de la población en torno al promedio y con una varianza conocida.\n\nSimétricas y con un solo punto de elevación\nLa pendiente es más fuerte cerca del centro, y se suaviza hacia los extremos\nCoinciden al centro el promedio, la mediana y la moda\nLa desviación estandar expresa su dispersión.\nEstablece áreas o proporciones bajo la curva en base a desviaciones estándar del promedio."
  },
  {
    "objectID": "assignment/01-practico.html#distribución-normal-estándar",
    "href": "assignment/01-practico.html#distribución-normal-estándar",
    "title": "Distribución Normal e Intervalos de Confianza",
    "section": "3.3. Distribución Normal Estándar",
    "text": "3.3. Distribución Normal Estándar\nLa distribución normal estándar es una distribución normal con una media de 0 y una desviación estándar de 1."
  },
  {
    "objectID": "assignment/01-practico.html#puntaje-z-y-estandarización-de-variables",
    "href": "assignment/01-practico.html#puntaje-z-y-estandarización-de-variables",
    "title": "Distribución Normal e Intervalos de Confianza",
    "section": "3.4. Puntaje Z y estandarización de variables",
    "text": "3.4. Puntaje Z y estandarización de variables\nAl estandarizar las variables (como en la Curva Normal Estándar) lo que hacemos es expresar el valor de una distribución en términos de desviaciones estándar basados en la distribución normal. Esto nos permite comparar distribuciones distintas.\nAl valor estandarizado lo llamamos puntaje Z, y corresponde a la cantidad de desviaciones estándar que nos alejamos del promedio (para cada variable con la que trabajemos)."
  },
  {
    "objectID": "assignment/01-practico.html#cálculo-de-probabilidades-con-puntaje-z",
    "href": "assignment/01-practico.html#cálculo-de-probabilidades-con-puntaje-z",
    "title": "Distribución Normal e Intervalos de Confianza",
    "section": "3.5. Cálculo de probabilidades con puntaje z",
    "text": "3.5. Cálculo de probabilidades con puntaje z\nLos valores estandarizados o puntajes Z además nos permiten conocer probabilidades.\nCon R es posible generar un conjunto de datos simulados con una distribución normal.\n\nx_values &lt;- seq(-4,4,length=1000)\ny_values &lt;- dnorm(x_values)\nplot(x_values,y_values,type=\"l\",xlab=\"Valor Z\",ylab=\"Probabilidad\",main=\"Distribución Normal\")\n\n\n\n\nPodemos preguntar qué parte de la curva cae por debajo de un valor particular. Por ejemplo, preguntaremos sobre el valor 0 antes de ejecutar el código. Piense ¿cuál debería ser la respuesta?\n\npnorm(q = 0)\n\n[1] 0.5\n\n\nPor tanto, la probabilidad (en una curva normal estándar) de obtener un valor igual o menor a 0 es de 0.5, es decir, del 50%, pero ¿por qué?\n\nPorque como la distribución normal estándar es simétrica al rededor de cero, la probabilidad de que sea menor o igual a cero es 0.5, es decir, el 50% de la distribución está por debajo de cero y el otro 50% está por encima de cero.\n\nEso lo podemos ver en el gráfico:\n\nplot(x_values,y_values,type=\"l\",xlab=\"Valor Z\",ylab=\"Probabilidad\",main=\"Distribución Normal\")\nabline(v=0)\n\n\n\n\nAhora probemos los valores Z de +1,96 y -1,96.\nSabemos que estos valores aproximados marcan el 2,5% superior e inferior de la distribución normal estándar. Esto corresponde a un alfa típico \\(\\alpha = 0,05\\) para una prueba de hipótesis de dos colas.\n\npnorm(q = 1.96, lower.tail=TRUE)\n\n[1] 0.9750021\n\n\nLa respuesta nos dice lo que ya sabemos: el 97,5% de la distribución normal ocurre por debajo del valor z de 1,96.\nPodemos agregar una línea al gráfico para mostrar dónde se usaría abline.\nEl 97,5% de la distribución queda por debajo de esta línea.\n\nplot(x_values, y_values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = 1.96)\n\n\n\n\ninteger(0)"
  },
  {
    "objectID": "assignment/01-practico.html#cálculo-de-intervalos-de-confianza",
    "href": "assignment/01-practico.html#cálculo-de-intervalos-de-confianza",
    "title": "Distribución Normal e Intervalos de Confianza",
    "section": "4.1. Cálculo de intervalos de confianza",
    "text": "4.1. Cálculo de intervalos de confianza\nEn el caso de nuestro vector aleatorio, un intervalo de confianza para la media se puede calcular de la siguiente manera:\n\n# Calcular un intervalo de confianza para la media\nintervalo_confianza &lt;- t.test(vector)$conf.int  # Intervalo de confianza del 95% para la media\nintervalo_confianza\n\n[1] 4.818567 5.543057\nattr(,\"conf.level\")\n[1] 0.95\n\n\nTambién podemos calcular intervalos de confianza para casos reales. Carguemos la base de datos que utilizaremos, que corresponde a un subset de la Encuesta Suplementaria de ingresos ESI para ocupados:\n\nload(url(\"https://github.com/cursos-metodos-facso/datos-ejemplos/raw/main/esi-2021-ocupados.rdata\"))\n\n\n\n\n\n\n\nNota\n\n\n\nRecordemos que podemos contar con bases de datos que tengan factor de expansión (ponderador) o no. Esta distinción se presenta cuando trabajamos con muestras simples o complejas. Al trabajar con muestras complejas debemos identificar cuál es la variable del ponderador e incorporarla en nuestro cálculo.\nEn esta guía practicaa trabajaremos sin factores de expansión o ponderadores.\n\n\n\nIC para Medias\nCalculemos un intervalo de confianza para la media de ingresos de personas ocupadas:\n\npsych::describe(esi$ing_t_p)\n\n   vars     n     mean       sd   median  trimmed      mad min      max\nX1    1 37124 586360.4 697362.9 405347.7 474473.1 255411.6   0 38206253\n      range skew kurtosis      se\nX1 38206253   12   402.32 3619.36\n\n\n\nPublish::ci.mean(esi$ing_t_p, alpha = 0.05)\n\n mean      CI-95%               \n 586360.41 [579266.37;593454.45]\n\n\nContamos con una media de ingresos de $586.360 como estimación puntual. Pero también podemos decir que con un 95% de confianza el parámetro poblacional se encontrará entre $579.266 y $593.454."
  }
]